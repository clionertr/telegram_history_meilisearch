"""
å‘½ä»¤å¤„ç†æ¨¡å—

æ­¤æ¨¡å—è´Ÿè´£å¤„ç†ç”¨æˆ·é€šè¿‡ Search Bot å‘é€çš„å‘½ä»¤ï¼ŒåŒ…æ‹¬ï¼š
1. åŸºæœ¬å‘½ä»¤ï¼š/start, /help
2. æœç´¢å‘½ä»¤ï¼š/search <å…³é”®è¯>
3. ç®¡ç†å‘˜å‘½ä»¤ï¼š/add_whitelist, /remove_whitelist
"""

import logging
import re
import asyncio
import time # Added for cache timestamping
from typing import List, Optional, Union, Dict, Any, Tuple
from datetime import datetime

from telethon import events, Button
from telethon.tl.types import User

from core.meilisearch_service import MeiliSearchService
from core.config_manager import ConfigManager
from .cache_service import SearchCacheService # Added
from search_bot.message_formatters import format_search_results, format_error_message, format_help_message, format_dialogs_list
from user_bot.client import UserBotClient

# é…ç½®æ—¥å¿—è®°å½•å™¨
logger = logging.getLogger(__name__)


class CommandHandlers:
    """
    å‘½ä»¤å¤„ç†å™¨ç±»
    
    è´Ÿè´£å¤„ç†ç”¨æˆ·é€šè¿‡ Search Bot å‘é€çš„å„ç±»å‘½ä»¤
    """

    def __init__(
        self,
        client,
        meilisearch_service: MeiliSearchService,
        config_manager: ConfigManager,
        admin_ids: List[int],
        userbot_restart_event: Optional[asyncio.Event] = None
    ) -> None:
        """
        åˆå§‹åŒ–å‘½ä»¤å¤„ç†å™¨
        
        Args:
            client: Telethon å®¢æˆ·ç«¯
            meilisearch_service: Meilisearch æœåŠ¡å®ä¾‹
            config_manager: é…ç½®ç®¡ç†å™¨å®ä¾‹
            admin_ids: ç®¡ç†å‘˜ç”¨æˆ· ID åˆ—è¡¨
            userbot_restart_event: User Bot é‡å¯äº‹ä»¶ï¼Œç”¨äºè§¦å‘é‡å¯
        """
        self.client = client
        self.meilisearch_service = meilisearch_service
        self.config_manager = config_manager
        self.admin_ids = admin_ids
        self.userbot_restart_event = userbot_restart_event
        self.cache_service = SearchCacheService(config_manager)
        self.active_full_fetches: Dict[str, asyncio.Task] = {} # For managing async full-fetch tasks
        
        # æ³¨å†Œå‘½ä»¤å¤„ç†å‡½æ•°
        self.register_handlers()
        
        logger.info("å‘½ä»¤å¤„ç†å™¨å·²åˆå§‹åŒ–ï¼Œæœç´¢ç¼“å­˜æœåŠ¡å·²é…ç½®")
    
    def register_handlers(self) -> None:
        """
        æ³¨å†Œæ‰€æœ‰å‘½ä»¤å¤„ç†å‡½æ•°
        """
        # åŸºæœ¬å‘½ä»¤
        self.client.add_event_handler(
            self.start_command,
            events.NewMessage(pattern=r"^/start$")
        )
        
        self.client.add_event_handler(
            self.help_command,
            events.NewMessage(pattern=r"^/help$")
        )
        
        # æœç´¢å‘½ä»¤
        self.client.add_event_handler(
            self.search_command,
            events.NewMessage(pattern=r"^/search(?:\s+(.+))?$")
        )
        
        # ç®¡ç†å‘˜å‘½ä»¤
        self.client.add_event_handler(
            self.add_whitelist_command,
            events.NewMessage(pattern=r"^/add_whitelist(?:\s+(-?\d+))?$")
        )
        
        self.client.add_event_handler(
            self.remove_whitelist_command,
            events.NewMessage(pattern=r"^/remove_whitelist(?:\s+(-?\d+))?$")
        )
        
        # User Bot é…ç½®ç›¸å…³å‘½ä»¤
        self.client.add_event_handler(
            self.set_userbot_config_command,
            events.NewMessage(pattern=r"^/set_userbot_config(?:\s+(\S+))?(?:\s+(.+))?$")
        )
        
        self.client.add_event_handler(
            self.view_userbot_config_command,
            events.NewMessage(pattern=r"^/view_userbot_config$")
        )
        
        self.client.add_event_handler(
            self.restart_userbot_command,
            events.NewMessage(pattern=r"^/restart_userbot$")
        )
        
        # å¯¹è¯åˆ—è¡¨å‘½ä»¤
        self.client.add_event_handler(
            self.get_dialogs_command,
            events.NewMessage(pattern=r"^/get_dialogs$")
        )

        # æ–°å¢ï¼šå¤„ç†æ™®é€šæ–‡æœ¬æ¶ˆæ¯ä½œä¸ºæœç´¢ï¼ˆåº”åœ¨æ‰€æœ‰ç‰¹å®šå‘½ä»¤ä¹‹åæ³¨å†Œï¼‰
        self.client.add_event_handler(
            self.handle_plain_text_message,
            events.NewMessage(func=self._is_plain_text_and_not_command)
        )

        # Search Cache Admin Commands
        self.client.add_event_handler(
            self.view_search_config_command,
            events.NewMessage(pattern=r"^/view_search_config$")
        )
        self.client.add_event_handler(
            self.set_search_config_command,
            events.NewMessage(pattern=r"^/set_search_config(?:\s+(\S+))?(?:\s+(.+))?$")
        )
        self.client.add_event_handler(
            self.clear_search_cache_command,
            events.NewMessage(pattern=r"^/clear_search_cache$")
        )
        
        logger.info("å·²æ³¨å†Œæ‰€æœ‰å‘½ä»¤å¤„ç†å‡½æ•°ï¼ŒåŒ…æ‹¬æ™®é€šæ–‡æœ¬æœç´¢å¤„ç†å™¨å’Œæœç´¢ç¼“å­˜ç®¡ç†å‘½ä»¤")
    
    def _is_plain_text_and_not_command(self, event) -> bool:
        """
        æ£€æŸ¥æ¶ˆæ¯æ˜¯å¦ä¸ºæ™®é€šæ–‡æœ¬æ¶ˆæ¯ï¼Œå¹¶ä¸”ä¸æ˜¯ä¸€ä¸ªå·²çŸ¥çš„å‘½ä»¤ã€‚
        åªå¤„ç†æ¥è‡ªç”¨æˆ·çš„æ¶ˆæ¯ï¼Œå¿½ç•¥é¢‘é“å¹¿æ’­ç­‰ã€‚
        """
        # ç¡®ä¿æ¶ˆæ¯æ¥è‡ªç”¨æˆ· (ä¸æ˜¯é¢‘é“è‡ªåŠ¨å‘å¸ƒç­‰)
        if not event.is_private and not event.is_group: # ç®€å•åˆ¤æ–­ï¼Œå¯æ ¹æ®éœ€æ±‚è°ƒæ•´
             if event.chat and hasattr(event.chat, 'broadcast') and event.chat.broadcast:
                 return False # æ˜¯é¢‘é“å¹¿æ’­

        if not event.message or not event.message.text:
            return False # æ²¡æœ‰æ–‡æœ¬å†…å®¹ (ä¾‹å¦‚å›¾ç‰‡ã€è´´çº¸)
        
        text = event.message.text.strip()
        if not text: # æ¶ˆæ¯ä¸ºç©ºæˆ–åªæœ‰ç©ºæ ¼
            return False

        # æ£€æŸ¥æ˜¯å¦ä»¥å·²çŸ¥å‘½ä»¤å‰ç¼€å¼€å¤´
        # æ³¨æ„ï¼šè¿™é‡Œçš„å‘½ä»¤åˆ—è¡¨åº”è¯¥ä¸ register_handlers ä¸­æ³¨å†Œçš„å‘½ä»¤ä¿æŒä¸€è‡´
        known_commands_patterns = [
            r"^/start$",
            r"^/help$",
            r"^/search(?:\s+(.+))?$",
            r"^/add_whitelist(?:\s+(-?\d+))?$",
            r"^/remove_whitelist(?:\s+(-?\d+))?$",
            r"^/set_userbot_config(?:\s+(\S+))?(?:\s+(.+))?$",
            r"^/view_userbot_config$",
            r"^/restart_userbot$",
            r"^/get_dialogs$",
            # Add new search config commands to prevent them being treated as plain text
            r"^/view_search_config$",
            r"^/set_search_config(?:\s+(\S+))?(?:\s+(.+))?$",
            r"^/clear_search_cache$"
        ]
        
        for pattern in known_commands_patterns:
            if re.match(pattern, text):
                return False # åŒ¹é…å·²çŸ¥å‘½ä»¤æ ¼å¼

        # è¿›ä¸€æ­¥æ’é™¤ä»»ä½•ä»¥ / å¼€å¤´çš„æ¶ˆæ¯ï¼Œä»¥é˜²æœ‰æœªæ˜ç¡®åˆ—å‡ºçš„å‘½ä»¤
        if text.startswith('/'):
            return False
            
        return True # æ˜¯æ™®é€šæ–‡æœ¬æ¶ˆæ¯ï¼Œä¸”ä¸æ˜¯å·²çŸ¥å‘½ä»¤

    async def handle_plain_text_message(self, event) -> None:
        """
        å¤„ç†æ™®é€šæ–‡æœ¬æ¶ˆæ¯ï¼Œå°†å…¶ä½œä¸ºæœç´¢æŸ¥è¯¢ã€‚
        """
        query = event.message.text.strip()
        # ç¡®ä¿æŸ¥è¯¢ä¸ä¸ºç©ºï¼ˆè™½ç„¶ _is_plain_text_and_not_command å·²ç»æ£€æŸ¥è¿‡ï¼‰
        if not query:
            return

        logger.info(f"æ¥æ”¶åˆ°æ™®é€šæ–‡æœ¬æ¶ˆæ¯ï¼Œå°†ä½œä¸ºæœç´¢æŸ¥è¯¢: '{query}' from user {(await event.get_sender()).id}")
        await self._perform_search(event, query, is_direct_search=True)

    async def is_admin(self, event) -> bool:
        """
        æ£€æŸ¥ç”¨æˆ·æ˜¯å¦ä¸ºç®¡ç†å‘˜
        
        Args:
            event: Telethon äº‹ä»¶å¯¹è±¡
            
        Returns:
            bool: ç”¨æˆ·æ˜¯å¦ä¸ºç®¡ç†å‘˜
        """
        sender = await event.get_sender()
        user_id = sender.id
        
        is_admin = user_id in self.admin_ids
        if not is_admin:
            logger.warning(f"ç”¨æˆ· {user_id} å°è¯•æ‰§è¡Œç®¡ç†å‘˜å‘½ä»¤ï¼Œä½†ä¸åœ¨ç®¡ç†å‘˜åˆ—è¡¨ä¸­")
        
        return is_admin
    
    async def start_command(self, event) -> None:
        """
        å¤„ç† /start å‘½ä»¤
        
        å‘é€æ¬¢è¿æ¶ˆæ¯å’ŒåŸºæœ¬ä½¿ç”¨è¯´æ˜
        
        Args:
            event: Telethon äº‹ä»¶å¯¹è±¡
        """
        try:
            sender = await event.get_sender()
            user_name = getattr(sender, 'first_name', 'User')
            
            # æ„å»ºæ¬¢è¿æ¶ˆæ¯
            welcome_message = (
                f"ğŸ‘‹ **ä½ å¥½ï¼Œ{user_name}ï¼**\n\n"
                f"æ¬¢è¿ä½¿ç”¨ Telegram ä¸­æ–‡å†å²æ¶ˆæ¯æœç´¢æœºå™¨äººã€‚\n\n"
                f"ä½ å¯ä»¥é€šè¿‡å‘é€ `/search å…³é”®è¯` æ¥æœç´¢å†å²æ¶ˆæ¯ã€‚\n"
                f"ä¾‹å¦‚ï¼š`/search Python æ•™ç¨‹`\n\n"
                f"å‘é€ `/help` è·å–æ›´è¯¦ç»†çš„ä½¿ç”¨è¯´æ˜å’Œé«˜çº§æœç´¢è¯­æ³•ã€‚"
            )
            
            await event.respond(welcome_message, parse_mode='md') # å¯ç”¨ Markdown
            logger.info(f"å·²å‘é€æ¬¢è¿æ¶ˆæ¯ç»™ç”¨æˆ· {sender.id}")
            
        except Exception as e:
            logger.error(f"å¤„ç† /start å‘½ä»¤æ—¶å‡ºé”™: {e}")
            await event.respond("ğŸ˜• å¯åŠ¨æœºå™¨äººæ—¶å‡ºç°é”™è¯¯ï¼Œè¯·ç¨åå†è¯•ã€‚")
    
    async def help_command(self, event) -> None:
        """
        å¤„ç† /help å‘½ä»¤
        
        å‘é€è¯¦ç»†çš„å¸®åŠ©ä¿¡æ¯ï¼ŒåŒ…æ‹¬æœç´¢è¯­æ³•
        
        Args:
            event: Telethon äº‹ä»¶å¯¹è±¡
        """
        try:
            help_message = format_help_message()
            await event.respond(help_message, parse_mode='md') # å¯ç”¨ Markdown
            logger.info(f"å·²å‘é€å¸®åŠ©æ¶ˆæ¯ç»™ç”¨æˆ· {(await event.get_sender()).id}")
            
        except Exception as e:
            logger.error(f"å¤„ç† /help å‘½ä»¤æ—¶å‡ºé”™: {e}")
            await event.respond("ğŸ˜• è·å–å¸®åŠ©ä¿¡æ¯æ—¶å‡ºç°é”™è¯¯ï¼Œè¯·ç¨åå†è¯•ã€‚")

    async def _get_results_from_meili(self,
                                      parsed_query: str,
                                      filters: Optional[str],
                                      sort: List[str],
                                      page: int,
                                      hits_per_page: int) -> Dict[str, Any]:
        """Helper function to call MeiliSearch and return results."""
        return self.meilisearch_service.search(
            query=parsed_query,
            filters=filters,
            sort=sort,
            page=page,
            hits_per_page=hits_per_page
        )

    async def _fetch_all_results_async(self, cache_key: str, parsed_query: str, filters_dict: Optional[Dict[str, Any]], meili_filters: Optional[str], sort_options: List[str], total_hits_estimate: int):
        """
        å¼‚æ­¥è·å–æ‰€æœ‰æœç´¢ç»“æœå¹¶æ›´æ–°ç¼“å­˜ã€‚
        """
        try:
            logger.info(f"åå°ä»»åŠ¡å¼€å§‹: ä¸º key='{cache_key}' è·å–å…¨éƒ¨ {total_hits_estimate} æ¡ç»“æœã€‚")
            # Fetch all results. MeiliSearch's limit parameter can be high.
            # We assume meilisearch_service.search handles pagination if we ask for all hits.
            # Let's fetch all in one go if total_hits_estimate is manageable, e.g., < 1000.
            # Otherwise, we might need a loop here, but for now, try to get all.
            # The MeiliSearch Python SDK's `search` method with `hits_per_page` set to total_hits
            # should ideally return all results if the server limit allows.
            # If total_hits_estimate is very large, this might be slow or hit server limits.
            # For now, we'll request all items.
            
            # A more robust approach for very large result sets would be to fetch in chunks
            # and append, but that complicates cache updates significantly.
            # Let's assume total_hits_estimate is within a reasonable limit for a single fetch.
            # The default limit for MeiliSearch is 1000 per query if not specified otherwise by `hitsPerPage`.
            
            all_results_data = []
            # If total_hits_estimate is 0, no need to fetch
            if total_hits_estimate == 0:
                logger.info(f"åå°ä»»åŠ¡: key='{cache_key}', æ€»å‘½ä¸­æ•°ä¸º0ï¼Œæ— éœ€è·å–ã€‚")
                self.cache_service.update_cache_to_complete(parsed_query, filters_dict, [], 0)
                return

            # Fetch all results. The MeiliSearch client handles pagination internally if we set a high limit.
            # We'll fetch all results in one go.
            # The `search` method in MeiliSearchService already handles `page` and `hits_per_page`.
            # To get all, we can set hits_per_page to total_hits and page to 1.
            # However, MeiliSearch has a default limit (often 1000).
            # For simplicity, we'll fetch up to a practical limit (e.g. 1000) for the "full" cache.
            # If total_hits > 1000, the "full" cache will contain the first 1000.
            # This is a pragmatic choice to avoid excessive memory/network for huge result sets.
            # The user can still paginate through MeiliSearch directly via callbacks if needed beyond this.
            
            fetch_limit = min(total_hits_estimate, 1000) # Cap full fetch at 1000 for now

            if fetch_limit > 0:
                full_search_results_obj = await self._get_results_from_meili(
                    parsed_query=parsed_query,
                    filters=meili_filters,
                    sort=sort_options,
                    page=1, # Get all from the first page
                    hits_per_page=fetch_limit # Request all (up to the cap)
                )
                all_results_data = full_search_results_obj.get('hits', [])
            
            self.cache_service.update_cache_to_complete(parsed_query, filters_dict, all_results_data, total_hits_estimate)
            logger.info(f"åå°ä»»åŠ¡å®Œæˆ: key='{cache_key}' çš„ç¼“å­˜å·²æ›´æ–°ä¸º {len(all_results_data)} æ¡å®Œæ•´ç»“æœ (æ€»é¢„ä¼° {total_hits_estimate})ã€‚")

        except Exception as e:
            logger.error(f"åå°ä»»åŠ¡å¤±è´¥: key='{cache_key}' è·å–å…¨éƒ¨ç»“æœæ—¶å‡ºé”™: {e}", exc_info=True)
            # Optionally, remove the partial cache entry or mark it as failed?
            # For now, the partial entry will remain until TTL.
        finally:
            if cache_key in self.active_full_fetches:
                del self.active_full_fetches[cache_key]

    async def _perform_search(self, event, query: str, page: int = 1, is_direct_search: bool = False) -> None:
        """
        æ‰§è¡Œæœç´¢æ“ä½œå¹¶å›å¤ç»“æœã€‚é›†æˆäº†ç¼“å­˜é€»è¾‘ã€‚
        
        Args:
            event: Telethon äº‹ä»¶å¯¹è±¡ã€‚
            query: æœç´¢å…³é”®è¯ã€‚
            page: è¯·æ±‚çš„é¡µç  (ç”¨äºåˆ†é¡µ)ã€‚
            is_direct_search: æ˜¯å¦ä¸ºç›´æ¥æ— å‘½ä»¤æœç´¢ã€‚
        """
        try:
            sender_id = (await event.get_sender()).id
            logger.info(f"ç”¨æˆ· {sender_id} æœç´¢: '{query}', é¡µç : {page}")

            parsed_query, filters_dict = self._parse_advanced_syntax(query)
            meili_filters = self._build_meilisearch_filters(filters_dict) if filters_dict else None
            
            hits_per_page = 5  # Standard items per page for display
            sort_options = ["date:desc"] # Default sort

            cached_data_entry = None
            if self.cache_service.is_cache_enabled():
                cached_data_entry = self.cache_service.get_from_cache(parsed_query, filters_dict)

            if cached_data_entry:
                cached_results, is_partial, total_hits_from_cache, fetch_ts = cached_data_entry
                logger.info(f"ç¼“å­˜å‘½ä¸­ for query '{parsed_query}'. Partial: {is_partial}, Total Hits: {total_hits_from_cache}")

                if not is_partial: # Full data in cache
                    results_to_format = {
                        'hits': cached_results[ (page - 1) * hits_per_page : page * hits_per_page ],
                        'query': parsed_query,
                        'processingTimeMs': 0, # From cache
                        'estimatedTotalHits': total_hits_from_cache
                    }
                    total_pages = (total_hits_from_cache + hits_per_page - 1) // hits_per_page if total_hits_from_cache > 0 else 0
                    formatted_message, buttons = format_search_results(results_to_format, page, total_pages, query_original=query)
                    await event.respond(formatted_message, buttons=buttons, parse_mode='md')
                    logger.info(f"å·²ä»å®Œæ•´ç¼“å­˜å‘ç”¨æˆ· {sender_id} å‘é€ç¬¬ {page} é¡µç»“æœ")
                    return

                # Partial data in cache
                if total_hits_from_cache is not None:
                    # Check if requested page is within the initial fetch
                    if page * hits_per_page <= len(cached_results):
                        results_to_format = {
                            'hits': cached_results[ (page - 1) * hits_per_page : page * hits_per_page ],
                            'query': parsed_query,
                            'processingTimeMs': 0, # From cache
                            'estimatedTotalHits': total_hits_from_cache
                        }
                        total_pages = (total_hits_from_cache + hits_per_page - 1) // hits_per_page if total_hits_from_cache > 0 else 0
                        formatted_message, buttons = format_search_results(results_to_format, page, total_pages, query_original=query)
                        await event.respond(formatted_message, buttons=buttons, parse_mode='md')
                        logger.info(f"å·²ä»éƒ¨åˆ†ç¼“å­˜ (åˆå§‹è·å–éƒ¨åˆ†) å‘ç”¨æˆ· {sender_id} å‘é€ç¬¬ {page} é¡µç»“æœ")
                        return
                    else:
                        # Requested page is beyond initial fetch.
                        # Check if full fetch is ongoing or completed.
                        cache_key_for_async = self.cache_service._generate_cache_key(parsed_query, filters_dict) # pylint: disable=protected-access
                        if cache_key_for_async in self.active_full_fetches or (fetch_ts is not None and time.time() - fetch_ts < 60): # Task running or recently started (give it 60s)
                            # Chosen:æ–¹æ¡ˆA (æç¤ºç”¨æˆ·ç­‰å¾…) for pagination beyond initial while async fetch is running
                            await event.respond("â³ æ­£åœ¨åŠ è½½æ›´å¤šç»“æœï¼Œè¯·ç¨å€™ç‰‡åˆ»å†å°è¯•ç¿»é¡µ...", parse_mode='md')
                            logger.info(f"ç”¨æˆ· {sender_id} è¯·æ±‚çš„é¡µé¢è¶…å‡ºåˆå§‹ç¼“å­˜ï¼Œåå°ä»»åŠ¡ä»åœ¨è¿›è¡Œä¸­ã€‚")
                            # Record this choice in activeContext.md
                            # Decision: For pagination requests beyond the initial cached set, while a background
                            # full-fetch task is active (or was very recently initiated), we will inform the user
                            # that more results are loading and ask them to wait. This avoids hitting MeiliSearch
                            # again for a page that will soon be part of the complete cache entry, and provides
                            # a better UX than an immediate (potentially partial or inconsistent) result.
                            # The alternative of fetching this specific page directly from MeiliSearch might lead
                            # to this page not being part of the "all_results_data" when the background task completes,
                            # or requiring complex merging logic.
                            return
                        else:
                            # Full fetch might have completed and updated the cache, or failed/timed out.
                            # Re-check cache, it might be complete now.
                            fresh_cached_entry = self.cache_service.get_from_cache(parsed_query, filters_dict)
                            if fresh_cached_entry and not fresh_cached_entry[1]: # is_partial is False
                                cached_results_updated, _, total_hits_updated, _ = fresh_cached_entry
                                results_to_format = {
                                    'hits': cached_results_updated[ (page - 1) * hits_per_page : page * hits_per_page ],
                                    'query': parsed_query,
                                    'processingTimeMs': 0,
                                    'estimatedTotalHits': total_hits_updated
                                }
                                total_pages = (total_hits_updated + hits_per_page - 1) // hits_per_page if total_hits_updated > 0 else 0
                                formatted_message, buttons = format_search_results(results_to_format, page, total_pages, query_original=query)
                                await event.respond(formatted_message, buttons=buttons, parse_mode='md')
                                logger.info(f"å·²ä»æ›´æ–°åçš„å®Œæ•´ç¼“å­˜å‘ç”¨æˆ· {sender_id} å‘é€ç¬¬ {page} é¡µç»“æœ")
                                return
                            # If still partial or not found, proceed to fetch from Meili (should ideally not happen if logic is correct)
                            logger.warning(f"ç¼“å­˜çŠ¶æ€å¼‚å¸¸ for {parsed_query} after async check, proceeding to MeiliSearch for page {page}")


            # Cache miss or only partial data that doesn't cover the page and async fetch not active/helpful
            # This part is primarily for the first time a search is made (page=1)
            if page == 1: # Only do initial + async fetch on the first page request
                status_message = await event.respond("ğŸ” æ­£åœ¨æœç´¢ï¼Œè¯·ç¨å€™...", parse_mode='md')
                
                initial_fetch_count = self.cache_service.get_initial_fetch_count()
                
                # Stage 1: Initial Fetch
                initial_results_obj = await self._get_results_from_meili(
                    parsed_query, meili_filters, sort_options, 1, initial_fetch_count
                )
                initial_hits_data = initial_results_obj.get('hits', [])
                estimated_total_hits = initial_results_obj.get('estimatedTotalHits', 0)

                # Store initial results in cache
                full_fetch_ts = None
                if self.cache_service.is_cache_enabled():
                    if estimated_total_hits > len(initial_hits_data):
                        full_fetch_ts = time.time() # Mark time if async fetch will be needed
                    self.cache_service.store_in_cache(
                        parsed_query, filters_dict, initial_hits_data, estimated_total_hits,
                        is_partial=(estimated_total_hits > len(initial_hits_data)),
                        full_fetch_initiated_timestamp=full_fetch_ts
                    )

                # Format and send initial results
                total_pages_for_initial = (estimated_total_hits + hits_per_page - 1) // hits_per_page if estimated_total_hits > 0 else 0
                formatted_message, buttons = format_search_results(initial_results_obj, 1, total_pages_for_initial, query_original=query)
                
                try:
                    await status_message.edit(formatted_message, buttons=buttons, parse_mode='md')
                except Exception: # If edit fails (e.g. message too old)
                    await event.respond(formatted_message, buttons=buttons, parse_mode='md')
                logger.info(f"å·²å‘ç”¨æˆ· {sender_id} å‘é€åˆå§‹ {len(initial_hits_data)} æ¡æœç´¢ç»“æœ (æ€»å…± {estimated_total_hits} æ¡)")

                # Stage 2: Asynchronous Full Fetch (if needed)
                if self.cache_service.is_cache_enabled() and estimated_total_hits > len(initial_hits_data):
                    cache_key_for_async = self.cache_service._generate_cache_key(parsed_query, filters_dict) # pylint: disable=protected-access
                    if cache_key_for_async not in self.active_full_fetches:
                        logger.info(f"å¯åŠ¨åå°ä»»åŠ¡: ä¸º key='{cache_key_for_async}' è·å–å‰©ä½™ç»“æœã€‚")
                        task = asyncio.create_task(
                            self._fetch_all_results_async(
                                cache_key_for_async, parsed_query, filters_dict, meili_filters, sort_options, estimated_total_hits
                            )
                        )
                        self.active_full_fetches[cache_key_for_async] = task
                    else:
                        logger.info(f"åå°ä»»åŠ¡å·²åœ¨è¿è¡Œ: key='{cache_key_for_async}'")
                return # Initial results sent, async fetch (if any) started.

            else: # page > 1 and data not sufficiently in cache
                # This case means user is asking for a subsequent page,
                # but the cache (even after checking for async completion) doesn't have it.
                # This might happen if TTL expired, or cache was cleared, or async failed silently.
                # For robustness, fetch this specific page directly from MeiliSearch.
                # This page won't be part of the "full_fetch" logic if it runs later for the same query.
                logger.warning(f"ç¼“å­˜æœªå‘½ä¸­æˆ–æ•°æ®ä¸è¶³ (é¡µç  {page}) for '{parsed_query}'. ç›´æ¥ä» MeiliSearch è·å–ã€‚")
                status_message = await event.respond(f"ğŸ” æ­£åœ¨åŠ è½½ç¬¬ {page} é¡µï¼Œè¯·ç¨å€™...", parse_mode='md')
                
                page_specific_results_obj = await self._get_results_from_meili(
                    parsed_query, meili_filters, sort_options, page, hits_per_page
                )
                estimated_total_hits = page_specific_results_obj.get('estimatedTotalHits', 0) # Re-confirm total
                total_pages = (estimated_total_hits + hits_per_page - 1) // hits_per_page if estimated_total_hits > 0 else 0
                
                formatted_message, buttons = format_search_results(page_specific_results_obj, page, total_pages, query_original=query)
                try:
                    await status_message.edit(formatted_message, buttons=buttons, parse_mode='md')
                except Exception:
                    await event.respond(formatted_message, buttons=buttons, parse_mode='md')
                logger.info(f"å·²ç›´æ¥ä» MeiliSearch å‘ç”¨æˆ· {sender_id} å‘é€ç¬¬ {page} é¡µç»“æœ")
                return

        except Exception as e:
            logger.error(f"æ‰§è¡Œæœç´¢æ—¶å‡ºé”™ (query: {query}, page: {page}): {e}", exc_info=True)
            error_message = format_error_message(str(e))
            # Try to edit if status_message exists, otherwise respond
            try:
                if 'status_message' in locals() and status_message:
                    await status_message.edit(error_message, parse_mode='md')
                else:
                    await event.respond(error_message, parse_mode='md')
            except Exception:
                 await event.respond(error_message, parse_mode='md')

    async def search_command(self, event) -> None:
        """
        å¤„ç† /search å‘½ä»¤
        
        æ‰§è¡Œæœç´¢å¹¶è¿”å›ç»“æœ
        
        Args:
            event: Telethon äº‹ä»¶å¯¹è±¡
        """
        message_text = event.message.text
        match = re.match(r"^/search(?:\s+(.+))?$", message_text)
        
        if not match or not match.group(1):
            await event.respond("è¯·æä¾›æœç´¢å…³é”®è¯ï¼Œä¾‹å¦‚ï¼š`/search Python æ•™ç¨‹`\nå‘é€ `/help` è·å–æ›´å¤šä½¿ç”¨è¯´æ˜ã€‚")
            return
        
        query = match.group(1).strip()
        # For /search command, it's always the first page initially
        await self._perform_search(event, query, page=1)
    
    def _parse_advanced_syntax(self, query: str) -> Tuple[str, Dict[str, Any]]:
        """
        è§£æé«˜çº§æœç´¢è¯­æ³•
        
        æ”¯æŒçš„è¯­æ³•:
        - ç²¾ç¡®çŸ­è¯­: "å…³é”®çŸ­è¯­"
        - ç±»å‹ç­›é€‰: type:ç±»å‹ (user/group/channel)
        - æ—¶é—´ç­›é€‰: date:èµ·å§‹_ç»“æŸ (YYYY-MM-DD_YYYY-MM-DD)
        
        Args:
            query: åŸå§‹æŸ¥è¯¢å­—ç¬¦ä¸²
            
        Returns:
            Tuple[str, Dict[str, Any]]: å¤„ç†åçš„æŸ¥è¯¢å’Œè¿‡æ»¤æ¡ä»¶å­—å…¸
        """
        # åˆå§‹åŒ–ç»“æœ
        filters = {}
        
        # å¤„ç†ç±»å‹ç­›é€‰
        type_match = re.search(r'type:(\w+)', query)
        if type_match:
            chat_type = type_match.group(1).lower()
            if chat_type in ['user', 'group', 'channel']:
                filters['chat_type'] = chat_type
                # ä»æŸ¥è¯¢ä¸­ç§»é™¤ type: éƒ¨åˆ†
                query = re.sub(r'type:\w+', '', query).strip()
        
        # å¤„ç†æ—¶é—´ç­›é€‰
        date_match = re.search(r'date:(\d{4}-\d{2}-\d{2})(?:_(\d{4}-\d{2}-\d{2}))?', query)
        if date_match:
            start_date = date_match.group(1)
            end_date = date_match.group(2) if date_match.group(2) else datetime.now().strftime('%Y-%m-%d')
            
            # è½¬æ¢ä¸º Unix æ—¶é—´æˆ³
            try:
                start_timestamp = int(datetime.strptime(start_date, '%Y-%m-%d').timestamp())
                # å¯¹äºç»“æŸæ—¥æœŸï¼Œè®¾ç½®ä¸ºå½“å¤©çš„æœ€åä¸€ç§’
                end_timestamp = int(datetime.strptime(end_date + ' 23:59:59', '%Y-%m-%d %H:%M:%S').timestamp())
                
                filters['date_range'] = {
                    'start': start_timestamp,
                    'end': end_timestamp
                }
                
                # ä»æŸ¥è¯¢ä¸­ç§»é™¤ date: éƒ¨åˆ†
                query = re.sub(r'date:\d{4}-\d{2}-\d{2}(?:_\d{4}-\d{2}-\d{2})?', '', query).strip()
            except ValueError as e:
                logger.warning(f"æ—¥æœŸè§£æé”™è¯¯: {e}")
        
        # æ¸…ç†æŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œç§»é™¤å¤šä½™ç©ºæ ¼
        clean_query = re.sub(r'\s+', ' ', query).strip()
        
        return clean_query, filters
    
    def _build_meilisearch_filters(self, filters_dict: Dict[str, Any]) -> str:
        """
        æ„å»º Meilisearch è¿‡æ»¤æ¡ä»¶å­—ç¬¦ä¸²
        
        Args:
            filters_dict: è¿‡æ»¤æ¡ä»¶å­—å…¸
            
        Returns:
            str: Meilisearch è¿‡æ»¤æ¡ä»¶å­—ç¬¦ä¸²
        """
        filter_parts = []
        
        # å¤„ç†èŠå¤©ç±»å‹è¿‡æ»¤
        if 'chat_type' in filters_dict:
            filter_parts.append(f"chat_type = '{filters_dict['chat_type']}'")
        
        # å¤„ç†æ—¥æœŸèŒƒå›´è¿‡æ»¤
        if 'date_range' in filters_dict:
            date_range = filters_dict['date_range']
            filter_parts.append(f"date >= {date_range['start']} AND date <= {date_range['end']}")
        
        # ç»„åˆæ‰€æœ‰è¿‡æ»¤æ¡ä»¶
        return ' AND '.join(filter_parts) if filter_parts else None
    
    async def add_whitelist_command(self, event) -> None:
        """
        å¤„ç† /add_whitelist å‘½ä»¤ (ç®¡ç†å‘˜æƒé™)
        
        æ·»åŠ èŠå¤©åˆ°ç™½åå•
        
        Args:
            event: Telethon äº‹ä»¶å¯¹è±¡
        """
        try:
            # æ£€æŸ¥æƒé™
            if not await self.is_admin(event):
                await event.respond("âš ï¸ æ­¤å‘½ä»¤éœ€è¦ç®¡ç†å‘˜æƒé™ã€‚")
                return
            
            # è·å– chat_id
            message_text = event.message.text
            match = re.match(r"^/add_whitelist(?:\s+(-?\d+))?$", message_text)
            
            if not match or not match.group(1):
                await event.respond("è¯·æä¾›è¦æ·»åŠ çš„ chat_idï¼Œä¾‹å¦‚ï¼š`/add_whitelist -1001234567890`")
                return
            
            chat_id = int(match.group(1))
            
            # æ·»åŠ åˆ°ç™½åå•
            success = self.config_manager.add_to_whitelist(chat_id)
            
            if success:
                await event.respond(f"âœ… å·²æˆåŠŸå°† chat_id `{chat_id}` æ·»åŠ åˆ°ç™½åå•ã€‚", parse_mode='md') # å¯ç”¨ Markdown
                logger.info(f"ç®¡ç†å‘˜ {(await event.get_sender()).id} æ·»åŠ  {chat_id} åˆ°ç™½åå•")
            else:
                await event.respond(f"â„¹ï¸ chat_id `{chat_id}` å·²åœ¨ç™½åå•ä¸­ï¼Œæ— éœ€é‡å¤æ·»åŠ ã€‚", parse_mode='md') # å¯ç”¨ Markdown
            
        except Exception as e:
            logger.error(f"å¤„ç† /add_whitelist å‘½ä»¤æ—¶å‡ºé”™: {e}")
            await event.respond(f"âš ï¸ æ·»åŠ ç™½åå•æ—¶å‡ºç°é”™è¯¯: {str(e)}")
    
    async def remove_whitelist_command(self, event) -> None:
        """
        å¤„ç† /remove_whitelist å‘½ä»¤ (ç®¡ç†å‘˜æƒé™)
        
        ä»ç™½åå•ç§»é™¤èŠå¤©
        
        Args:
            event: Telethon äº‹ä»¶å¯¹è±¡
        """
        try:
            # æ£€æŸ¥æƒé™
            if not await self.is_admin(event):
                await event.respond("âš ï¸ æ­¤å‘½ä»¤éœ€è¦ç®¡ç†å‘˜æƒé™ã€‚")
                return
            
            # è·å– chat_id
            message_text = event.message.text
            match = re.match(r"^/remove_whitelist(?:\s+(-?\d+))?$", message_text)
            
            if not match or not match.group(1):
                await event.respond("è¯·æä¾›è¦ç§»é™¤çš„ chat_idï¼Œä¾‹å¦‚ï¼š`/remove_whitelist -1001234567890`")
                return
            
            chat_id = int(match.group(1))
            
            # ä»ç™½åå•ç§»é™¤
            success = self.config_manager.remove_from_whitelist(chat_id)
            
            if success:
                await event.respond(f"âœ… å·²æˆåŠŸå°† chat_id `{chat_id}` ä»ç™½åå•ç§»é™¤ã€‚", parse_mode='md') # å¯ç”¨ Markdown
                logger.info(f"ç®¡ç†å‘˜ {(await event.get_sender()).id} ä»ç™½åå•ç§»é™¤ {chat_id}")
            else:
                await event.respond(f"â„¹ï¸ chat_id `{chat_id}` ä¸åœ¨ç™½åå•ä¸­ï¼Œæ— éœ€ç§»é™¤ã€‚", parse_mode='md') # å¯ç”¨ Markdown
            
        except Exception as e:
            logger.error(f"å¤„ç† /remove_whitelist å‘½ä»¤æ—¶å‡ºé”™: {e}")
            await event.respond(f"âš ï¸ ç§»é™¤ç™½åå•æ—¶å‡ºç°é”™è¯¯: {str(e)}")
            
    async def set_userbot_config_command(self, event) -> None:
        """
        å¤„ç† /set_userbot_config å‘½ä»¤ (ç®¡ç†å‘˜æƒé™)
        
        è®¾ç½® User Bot é…ç½®é¡¹
        
        Args:
            event: Telethon äº‹ä»¶å¯¹è±¡
        """
        try:
            # æ£€æŸ¥æƒé™
            if not await self.is_admin(event):
                await event.respond("âš ï¸ æ­¤å‘½ä»¤éœ€è¦ç®¡ç†å‘˜æƒé™ã€‚")
                return
            
            # è·å–å‚æ•°
            message_text = event.message.text
            match = re.match(r"^/set_userbot_config(?:\s+(\S+))?(?:\s+(.+))?$", message_text)
            
            if not match or not match.group(1):
                help_text = """è¯·æä¾›è¦è®¾ç½®çš„é…ç½®é¡¹å’Œå€¼ï¼Œä¾‹å¦‚ï¼š
`/set_userbot_config USER_SESSION_NAME my_session`

å¯è®¾ç½®çš„é…ç½®é¡¹åŒ…æ‹¬ï¼š
- `USER_API_ID` - Telegram API ID
- `USER_API_HASH` - Telegram API Hash
- `USER_SESSION_NAME` - ä¼šè¯åç§°ï¼ˆå¦‚éœ€ä¿®æ”¹ï¼Œéœ€è¦é‡å¯ User Botï¼‰
- `USER_PROXY_URL` - ä»£ç†æœåŠ¡å™¨ URLï¼ˆå¦‚éœ€ä½¿ç”¨ï¼‰

âš ï¸ æ³¨æ„ï¼šä¿®æ”¹é…ç½®åï¼Œéœ€è¦ä½¿ç”¨ `/restart_userbot` å‘½ä»¤ä½¿é…ç½®ç”Ÿæ•ˆã€‚"""
            await event.respond(help_text, parse_mode='md') # å¯ç”¨ Markdown
            return
            
            key = match.group(1).upper()  # è½¬ä¸ºå¤§å†™
            if not match.group(2):
                await event.respond(f"è¯·æä¾› `{key}` çš„å€¼ï¼Œä¾‹å¦‚ï¼š`/set_userbot_config {key} value`")
                return
                
            value = match.group(2).strip()
            
            # æ·»åŠ USER_å‰ç¼€ï¼ˆå¦‚æœæ²¡æœ‰ï¼‰
            if not key.startswith("USER_"):
                key = f"USER_{key}"
                
            # è®¾ç½®é…ç½®
            self.config_manager.set_userbot_env(key, value)
            
            # å‘é€æˆåŠŸæ¶ˆæ¯
            await event.respond(f"âœ… å·²è®¾ç½® User Bot é…ç½®é¡¹ `{key}` = `{value if key != 'USER_API_HASH' else '******'}`\n\nä½¿ç”¨ `/restart_userbot` å‘½ä»¤ä½¿é…ç½®ç”Ÿæ•ˆã€‚", parse_mode='md') # å¯ç”¨ Markdown
            logger.info(f"ç®¡ç†å‘˜ {(await event.get_sender()).id} è®¾ç½® User Bot é…ç½®é¡¹ {key}")
            
        except Exception as e:
            logger.error(f"å¤„ç† /set_userbot_config å‘½ä»¤æ—¶å‡ºé”™: {e}")
            await event.respond(f"âš ï¸ è®¾ç½® User Bot é…ç½®æ—¶å‡ºç°é”™è¯¯: {str(e)}")
            
    async def view_userbot_config_command(self, event) -> None:
        """
        å¤„ç† /view_userbot_config å‘½ä»¤ (ç®¡ç†å‘˜æƒé™)
        
        æŸ¥çœ‹ User Bot å½“å‰é…ç½®
        
        Args:
            event: Telethon äº‹ä»¶å¯¹è±¡
        """
        try:
            # æ£€æŸ¥æƒé™
            if not await self.is_admin(event):
                await event.respond("âš ï¸ æ­¤å‘½ä»¤éœ€è¦ç®¡ç†å‘˜æƒé™ã€‚")
                return
            
            # è·å–é…ç½®
            config_dict = self.config_manager.get_userbot_config_dict(exclude_sensitive=True)
            
            if not config_dict:
                await event.respond("â„¹ï¸ User Bot å°šæœªé…ç½®ä»»ä½•ç¯å¢ƒå˜é‡ã€‚ä½¿ç”¨ `/set_userbot_config` å‘½ä»¤è¿›è¡Œé…ç½®ã€‚")
                return
                
            # æ ¼å¼åŒ–é…ç½®ä¿¡æ¯
            config_text = "ğŸ“ **User Bot å½“å‰é…ç½®**\n\n"
            for key, value in config_dict.items():
                config_text += f"- `{key}` = `{value}`\n"
                
            config_text += "\nä½¿ç”¨ `/set_userbot_config <key> <value>` ä¿®æ”¹é…ç½®ï¼Œä½¿ç”¨ `/restart_userbot` ä½¿é…ç½®ç”Ÿæ•ˆã€‚"
            
            await event.respond(config_text, parse_mode='md') # å¯ç”¨ Markdown
            logger.info(f"ç®¡ç†å‘˜ {(await event.get_sender()).id} æŸ¥çœ‹ User Bot é…ç½®")
            
        except Exception as e:
            logger.error(f"å¤„ç† /view_userbot_config å‘½ä»¤æ—¶å‡ºé”™: {e}")
            await event.respond(f"âš ï¸ æŸ¥çœ‹ User Bot é…ç½®æ—¶å‡ºç°é”™è¯¯: {str(e)}")
            
    async def restart_userbot_command(self, event) -> None:
        """
        å¤„ç† /restart_userbot å‘½ä»¤ (ç®¡ç†å‘˜æƒé™)
        
        é‡å¯ User Bot
        
        Args:
            event: Telethon äº‹ä»¶å¯¹è±¡
        """
        try:
            # æ£€æŸ¥æƒé™
            if not await self.is_admin(event):
                await event.respond("âš ï¸ æ­¤å‘½ä»¤éœ€è¦ç®¡ç†å‘˜æƒé™ã€‚")
                return
            
            # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¯äº‹ä»¶
            if not self.userbot_restart_event:
                await event.respond("âš ï¸ é‡å¯åŠŸèƒ½æœªåˆå§‹åŒ–ï¼Œæ— æ³•é‡å¯ User Botã€‚")
                logger.error("å°è¯•é‡å¯ User Botï¼Œä½† userbot_restart_event æœªåˆå§‹åŒ–")
                return
                
            # å‘é€é‡å¯æ¶ˆæ¯
            await event.respond("ğŸ”„ æ­£åœ¨é‡å¯ User Botï¼Œè¯·ç¨å€™...")
            logger.info(f"ç®¡ç†å‘˜ {(await event.get_sender()).id} è§¦å‘ User Bot é‡å¯")
            
            # è®¾ç½®é‡å¯äº‹ä»¶
            self.userbot_restart_event.set()
            
            # ç­‰å¾…ä¸€æ®µæ—¶é—´ï¼Œè®©é‡å¯è¿‡ç¨‹å®Œæˆ
            await asyncio.sleep(5)
            
            # å‘é€é‡å¯å®Œæˆæ¶ˆæ¯
            await event.respond("âœ… User Bot å·²é‡æ–°å¯åŠ¨ï¼Œæ–°é…ç½®å·²ç”Ÿæ•ˆã€‚")
            
        except Exception as e:
            logger.error(f"å¤„ç† /restart_userbot å‘½ä»¤æ—¶å‡ºé”™: {e}")
            await event.respond(f"âš ï¸ é‡å¯ User Bot æ—¶å‡ºç°é”™è¯¯: {str(e)}")

    async def get_dialogs_command(self, event) -> None:
        """
        å¤„ç† /get_dialogs å‘½ä»¤
        
        è·å–ç”¨æˆ·è´¦æˆ·ä¸‹çš„æ‰€æœ‰å¯¹è¯åˆ—è¡¨ï¼ŒåŒ…æ‹¬å¯¹è¯åç§°å’ŒID
        
        Args:
            event: Telethon äº‹ä»¶å¯¹è±¡
        """
        try:
            sender = await event.get_sender()
            sender_id = sender.id
            logger.info(f"ç”¨æˆ· {sender_id} è¯·æ±‚è·å–å¯¹è¯åˆ—è¡¨")
            
            # å‘é€å¤„ç†ä¸­çš„æ¶ˆæ¯
            status_message = await event.respond("ğŸ” æ­£åœ¨è·å–å¯¹è¯åˆ—è¡¨ï¼Œè¯·ç¨å€™...")
            
            dialogs_per_page = 15  # æ¯é¡µæ˜¾ç¤ºçš„å¯¹è¯æ•°é‡ï¼Œå¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´
            current_page = 1 # åˆå§‹è¯·æ±‚æ€»æ˜¯ç¬¬ä¸€é¡µ

            # è·å– UserBotClient å®ä¾‹
            try:
                userbot_client = UserBotClient()
                
                # è°ƒç”¨è·å–å¯¹è¯ä¿¡æ¯çš„æ–¹æ³•
                # æ³¨æ„ï¼šè¿™é‡Œè·å–çš„æ˜¯å®Œæ•´çš„å¯¹è¯åˆ—è¡¨
                all_dialogs_info = await userbot_client.get_dialogs_info()
                
                if not all_dialogs_info:
                    await status_message.edit("ğŸ“­ **å¯¹è¯åˆ—è¡¨ä¸ºç©º**\n\nå½“å‰è´¦æˆ·ä¸‹æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å¯¹è¯ã€‚", parse_mode='md')
                    logger.info(f"ç”¨æˆ· {sender_id} çš„å¯¹è¯åˆ—è¡¨ä¸ºç©º")
                    return

                total_dialogs = len(all_dialogs_info)
                total_pages = (total_dialogs + dialogs_per_page - 1) // dialogs_per_page
                if total_pages == 0: # Handle case with 0 dialogs, though caught by `if not all_dialogs_info`
                    total_pages = 1

                # æ ¼å¼åŒ–å¯¹è¯åˆ—è¡¨ï¼ˆç¬¬ä¸€é¡µï¼‰
                formatted_message, buttons = format_dialogs_list(
                    dialogs_info=all_dialogs_info,
                    current_page=current_page,
                    total_pages=total_pages,
                    items_per_page=dialogs_per_page
                )
                
                # æ›´æ–°æ¶ˆæ¯
                await status_message.edit(formatted_message, buttons=buttons, parse_mode='md')
                
                # è®°å½•æ—¥å¿—
                logger.info(f"å·²å‘ç”¨æˆ· {sender_id} å‘é€å¯¹è¯åˆ—è¡¨ç¬¬ {current_page}/{total_pages} é¡µï¼Œå…± {total_dialogs} ä¸ªå¯¹è¯")
                
                # åœ¨æ—¥å¿—ä¸­æ‰“å°å¯¹è¯ä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•ï¼Œå¯ä»¥è€ƒè™‘åªæ‰“å°éƒ¨åˆ†æˆ–æ‘˜è¦ï¼‰
                # logger.debug(f"å®Œæ•´å¯¹è¯åˆ—è¡¨è¯¦æƒ…: {all_dialogs_info}")
                
            except RuntimeError as e:
                # UserBot å®¢æˆ·ç«¯ç›¸å…³é”™è¯¯
                error_msg = "âš ï¸ User Bot æœªæ­£ç¡®åˆå§‹åŒ–æˆ–æœªè¿æ¥ï¼Œæ— æ³•è·å–å¯¹è¯åˆ—è¡¨ã€‚\n\nè¯·è”ç³»ç®¡ç†å‘˜æ£€æŸ¥ User Bot çŠ¶æ€ã€‚"
                await status_message.edit(error_msg, parse_mode='md')
                logger.error(f"UserBot å®¢æˆ·ç«¯é”™è¯¯: {e}")
                
            except Exception as e:
                # å…¶ä»–é”™è¯¯
                error_msg = f"âš ï¸ è·å–å¯¹è¯åˆ—è¡¨æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}\n\nè¯·ç¨åå†è¯•æˆ–è”ç³»ç®¡ç†å‘˜ã€‚"
                await status_message.edit(error_msg, parse_mode='md')
                logger.error(f"è·å–å¯¹è¯åˆ—è¡¨æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
                
        except Exception as e:
            logger.error(f"å¤„ç† /get_dialogs å‘½ä»¤æ—¶å‡ºé”™: {e}", exc_info=True)
            try:
                await event.respond("ğŸ˜• å¤„ç†è·å–å¯¹è¯åˆ—è¡¨è¯·æ±‚æ—¶å‡ºç°é”™è¯¯ï¼Œè¯·ç¨åå†è¯•ã€‚")
            except Exception:
                # å¦‚æœè¿å›å¤éƒ½å¤±è´¥äº†ï¼Œåªèƒ½è®°å½•æ—¥å¿—
                logger.error("æ— æ³•å‘é€é”™è¯¯å›å¤æ¶ˆæ¯")

    async def view_search_config_command(self, event) -> None:
        """å¤„ç† /view_search_config å‘½ä»¤ (ç®¡ç†å‘˜æƒé™)"""
        if not await self.is_admin(event):
            await event.respond("âš ï¸ æ­¤å‘½ä»¤éœ€è¦ç®¡ç†å‘˜æƒé™ã€‚")
            return
        try:
            config_text = "ğŸ” **æœç´¢ç¼“å­˜é…ç½®**\n\n"
            config_text += f"- å¯ç”¨ç¼“å­˜: `{self.config_manager.get_search_cache_enabled()}`\n"
            config_text += f"- ç¼“å­˜TTL (ç§’): `{self.config_manager.get_search_cache_ttl()}`\n"
            config_text += f"- åˆå§‹è·å–æ¡ç›®æ•°: `{self.config_manager.get_search_cache_initial_fetch_count()}`\n\n"
            
            cache_stats = self.cache_service.get_cache_stats()
            if cache_stats.get("enabled"):
                config_text += "**ç¼“å­˜çŠ¶æ€:**\n"
                config_text += f"- å½“å‰æ¡ç›®æ•°: `{cache_stats.get('currsize', 'N/A')}`\n"
                config_text += f"- æœ€å¤§æ¡ç›®æ•°: `{cache_stats.get('maxsize', 'N/A')}`\n"
            else:
                config_text += "**ç¼“å­˜çŠ¶æ€:** `å·²ç¦ç”¨`\n"
            
            await event.respond(config_text, parse_mode='md')
            logger.info(f"ç®¡ç†å‘˜ {(await event.get_sender()).id} æŸ¥çœ‹æœç´¢ç¼“å­˜é…ç½®")
        except Exception as e:
            logger.error(f"å¤„ç† /view_search_config å‘½ä»¤æ—¶å‡ºé”™: {e}")
            await event.respond(f"âš ï¸ æŸ¥çœ‹æœç´¢ç¼“å­˜é…ç½®æ—¶å‡ºç°é”™è¯¯: {str(e)}")

    async def set_search_config_command(self, event) -> None:
        """å¤„ç† /set_search_config å‘½ä»¤ (ç®¡ç†å‘˜æƒé™)"""
        if not await self.is_admin(event):
            await event.respond("âš ï¸ æ­¤å‘½ä»¤éœ€è¦ç®¡ç†å‘˜æƒé™ã€‚")
            return

        message_text = event.message.text
        match = re.match(r"^/set_search_config(?:\s+(\S+))?(?:\s+(.+))?$", message_text)

        if not match or not match.group(1) or not match.group(2):
            help_text = """è¯·æä¾›é…ç½®é¡¹å’Œå€¼ï¼Œä¾‹å¦‚ï¼š
`/set_search_config enable_search_cache true`

å¯è®¾ç½®çš„é…ç½®é¡¹:
- `enable_search_cache` (true/false)
- `search_cache_ttl_seconds` (æ•´æ•°, ä¾‹å¦‚ 3600)
- `search_cache_initial_fetch_count` (æ•´æ•°, ä¾‹å¦‚ 20)

æ›´æ”¹é…ç½®åï¼Œç¼“å­˜å°†é‡æ–°åˆå§‹åŒ–ã€‚"""
            await event.respond(help_text, parse_mode='md')
            return

        key = match.group(1).lower()
        value_str = match.group(2).strip()
        
        valid_keys = {
            "enable_search_cache": lambda v: v.lower() == 'true' or v.lower() == 'false',
            "search_cache_ttl_seconds": lambda v: v.isdigit(),
            "search_cache_initial_fetch_count": lambda v: v.isdigit()
        }

        if key not in valid_keys:
            await event.respond(f"âš ï¸ æ— æ•ˆçš„é…ç½®é¡¹: `{key}`ã€‚è¯·ä»å…è®¸çš„åˆ—è¡¨ä¸­é€‰æ‹©ã€‚")
            return

        try:
            processed_value: Union[bool, int, str]
            if key == "enable_search_cache":
                if value_str.lower() not in ['true', 'false']:
                    raise ValueError("enable_search_cache å¿…é¡»æ˜¯ true æˆ– false")
                processed_value = value_str.lower() == 'true'
            elif key in ["search_cache_ttl_seconds", "search_cache_initial_fetch_count"]:
                if not value_str.isdigit():
                    raise ValueError(f"{key} å¿…é¡»æ˜¯ä¸€ä¸ªæ•´æ•°")
                processed_value = int(value_str)
                if processed_value <= 0 and key != "search_cache_ttl_seconds": # TTL can be 0 for no expiry with maxsize
                     if processed_value <=0 and key == "search_cache_ttl_seconds" and self.config_manager.config.getint("SearchBot", "search_cache_ttl_seconds", fallback=1) == 0 : # allow 0 if already 0 (no expiry)
                         pass # allow 0 for TTL if it means no expiry based on cachetools
                     elif key == "search_cache_initial_fetch_count" and processed_value <=0:
                         raise ValueError(f"{key} å¿…é¡»å¤§äº 0")

            else: # Should not happen due to key check
                await event.respond(f"âš ï¸ æœªçŸ¥çš„é…ç½®é”®: {key}")
                return

            # Update ConfigParser in memory
            section = "SearchBot"
            if not self.config_manager.config.has_section(section):
                self.config_manager.config.add_section(section)
            
            self.config_manager.config.set(section, key, str(processed_value)) # Store as string in ini

            # Save to config.ini
            with open(self.config_manager.config_path, "w", encoding="utf-8") as f:
                self.config_manager.config.write(f)
            
            # Reload config in ConfigManager instance
            self.config_manager.load_config() # This re-reads the file into self.config
            self.config_manager._load_search_bot_config() # This updates the specific attributes

            # Re-initialize SearchCacheService with new config
            # Pass the existing maxsize if it was set, or default. For now, use default.
            current_maxsize = self.cache_service.get_cache_stats().get('maxsize', 200) if self.cache_service.is_cache_enabled() else 200

            self.cache_service = SearchCacheService(self.config_manager, maxsize=current_maxsize)
            self.active_full_fetches.clear() # Clear ongoing fetches

            await event.respond(f"âœ… é…ç½®é¡¹ `{key}` å·²æ›´æ–°ä¸º `{processed_value}`ã€‚\næœç´¢ç¼“å­˜å·²ä½¿ç”¨æ–°é…ç½®é‡æ–°åˆå§‹åŒ–ã€‚è¿›è¡Œä¸­çš„å¼‚æ­¥è·å–ä»»åŠ¡å·²æ¸…é™¤ã€‚", parse_mode='md')
            logger.info(f"ç®¡ç†å‘˜ {(await event.get_sender()).id} æ›´æ–°æœç´¢é…ç½®: {key} = {processed_value}")

        except ValueError as ve:
            await event.respond(f"âš ï¸ å€¼é”™è¯¯: {str(ve)}")
        except Exception as e:
            logger.error(f"å¤„ç† /set_search_config å‘½ä»¤æ—¶å‡ºé”™: {e}", exc_info=True)
            await event.respond(f"âš ï¸ æ›´æ–°æœç´¢é…ç½®æ—¶å‡ºç°ä¸¥é‡é”™è¯¯: {str(e)}")

    async def clear_search_cache_command(self, event) -> None:
        """å¤„ç† /clear_search_cache å‘½ä»¤ (ç®¡ç†å‘˜æƒé™)"""
        if not await self.is_admin(event):
            await event.respond("âš ï¸ æ­¤å‘½ä»¤éœ€è¦ç®¡ç†å‘˜æƒé™ã€‚")
            return
        try:
            self.cache_service.clear_cache()
            self.active_full_fetches.clear() # Clear any ongoing background fetch tasks
            await event.respond("âœ… æœç´¢ç¼“å­˜å·²æ¸…ç©ºï¼Œè¿›è¡Œä¸­çš„å¼‚æ­¥è·å–ä»»åŠ¡å·²æ¸…é™¤ã€‚")
            logger.info(f"ç®¡ç†å‘˜ {(await event.get_sender()).id} æ¸…ç©ºäº†æœç´¢ç¼“å­˜")
        except Exception as e:
            logger.error(f"å¤„ç† /clear_search_cache å‘½ä»¤æ—¶å‡ºé”™: {e}")
            await event.respond(f"âš ï¸ æ¸…ç©ºæœç´¢ç¼“å­˜æ—¶å‡ºç°é”™è¯¯: {str(e)}")

# è¾…åŠ©å‡½æ•°ï¼šåˆ›å»ºå‘½ä»¤å¤„ç†å™¨å¹¶æ³¨å†Œåˆ°å®¢æˆ·ç«¯
def setup_command_handlers(
    client,
    meilisearch_service: MeiliSearchService,
    config_manager: ConfigManager,
    admin_ids: List[int],
    userbot_restart_event: Optional[asyncio.Event] = None
) -> CommandHandlers:
    """
    åˆ›å»ºå‘½ä»¤å¤„ç†å™¨å¹¶å°†å…¶æ³¨å†Œåˆ°å®¢æˆ·ç«¯
    
    Args:
        client: Telethon å®¢æˆ·ç«¯
        meilisearch_service: Meilisearch æœåŠ¡å®ä¾‹
        config_manager: é…ç½®ç®¡ç†å™¨å®ä¾‹
        admin_ids: ç®¡ç†å‘˜ç”¨æˆ· ID åˆ—è¡¨
        userbot_restart_event: User Bot é‡å¯äº‹ä»¶ï¼Œç”¨äºè§¦å‘é‡å¯
        
    Returns:
        CommandHandlers: å‘½ä»¤å¤„ç†å™¨å®ä¾‹
    """
    handler = CommandHandlers(
        client=client,
        meilisearch_service=meilisearch_service,
        config_manager=config_manager,
        admin_ids=admin_ids,
        userbot_restart_event=userbot_restart_event
    )
    
    return handler