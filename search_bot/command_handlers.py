"""
å‘½ä»¤å¤„ç†æ¨¡å—

æ­¤æ¨¡å—è´Ÿè´£å¤„ç†ç”¨æˆ·é€šè¿‡ Search Bot å‘é€çš„å‘½ä»¤ï¼ŒåŒ…æ‹¬ï¼š
1. åŸºæœ¬å‘½ä»¤ï¼š/start, /help
2. æœç´¢å‘½ä»¤ï¼š/search <å…³é”®è¯>
3. ç®¡ç†å‘˜å‘½ä»¤ï¼š/add_whitelist, /remove_whitelist
"""

import logging
import re
import asyncio
import time # Added for cache timestamping
from typing import List, Optional, Union, Dict, Any, Tuple
from datetime import datetime

from telethon import events, Button
from telethon.tl.types import User

from core.meilisearch_service import MeiliSearchService
from core.config_manager import ConfigManager
from .cache_service import SearchCacheService # Added
from .dialogs_cache_service import DialogsCacheService # Added for dialogs caching
from search_bot.message_formatters import format_search_results, format_error_message, format_help_message, format_dialogs_list
from user_bot.client import UserBotClient

# é…ç½®æ—¥å¿—è®°å½•å™¨
logger = logging.getLogger(__name__)


class CommandHandlers:
    """
    å‘½ä»¤å¤„ç†å™¨ç±»
    
    è´Ÿè´£å¤„ç†ç”¨æˆ·é€šè¿‡ Search Bot å‘é€çš„å„ç±»å‘½ä»¤
    """

    def __init__(
        self,
        client,
        meilisearch_service: MeiliSearchService,
        config_manager: ConfigManager,
        admin_ids: List[int],
        userbot_restart_event: Optional[asyncio.Event] = None
    ) -> None:
        """
        åˆå§‹åŒ–å‘½ä»¤å¤„ç†å™¨
        
        Args:
            client: Telethon å®¢æˆ·ç«¯
            meilisearch_service: Meilisearch æœåŠ¡å®ä¾‹
            config_manager: é…ç½®ç®¡ç†å™¨å®ä¾‹
            admin_ids: ç®¡ç†å‘˜ç”¨æˆ· ID åˆ—è¡¨
            userbot_restart_event: User Bot é‡å¯äº‹ä»¶ï¼Œç”¨äºè§¦å‘é‡å¯
        """
        self.client = client
        self.meilisearch_service = meilisearch_service
        self.config_manager = config_manager
        self.admin_ids = admin_ids
        self.userbot_restart_event = userbot_restart_event
        self.cache_service = SearchCacheService(config_manager)
        self.dialogs_cache_service = DialogsCacheService(config_manager) # Added for dialogs caching
        self.active_full_fetches: Dict[str, asyncio.Task] = {} # For managing async full-fetch tasks
        
        # æ³¨å†Œå‘½ä»¤å¤„ç†å‡½æ•°
        self.register_handlers()
        
        logger.info("å‘½ä»¤å¤„ç†å™¨å·²åˆå§‹åŒ–ï¼Œæœç´¢ç¼“å­˜å’Œå¯¹è¯ç¼“å­˜æœåŠ¡å·²é…ç½®")
    
    def register_handlers(self) -> None:
        """
        æ³¨å†Œæ‰€æœ‰å‘½ä»¤å¤„ç†å‡½æ•°
        """
        # åŸºæœ¬å‘½ä»¤
        self.client.add_event_handler(
            self.start_command,
            events.NewMessage(pattern=r"^/start$")
        )
        
        self.client.add_event_handler(
            self.help_command,
            events.NewMessage(pattern=r"^/help$")
        )
        
        # æœç´¢å‘½ä»¤
        self.client.add_event_handler(
            self.search_command,
            events.NewMessage(pattern=r"^/search(?:\s+(.+))?$")
        )
        
        # ç®¡ç†å‘˜å‘½ä»¤
        self.client.add_event_handler(
            self.add_whitelist_command,
            events.NewMessage(pattern=r"^/add_whitelist(?:\s+(-?\d+))?$")
        )
        
        self.client.add_event_handler(
            self.remove_whitelist_command,
            events.NewMessage(pattern=r"^/remove_whitelist(?:\s+(-?\d+))?$")
        )
        
        # User Bot é…ç½®ç›¸å…³å‘½ä»¤
        self.client.add_event_handler(
            self.set_userbot_config_command,
            events.NewMessage(pattern=r"^/set_userbot_config(?:\s+(\S+))?(?:\s+(.+))?$")
        )
        
        self.client.add_event_handler(
            self.view_userbot_config_command,
            events.NewMessage(pattern=r"^/view_userbot_config$")
        )
        
        self.client.add_event_handler(
            self.restart_userbot_command,
            events.NewMessage(pattern=r"^/restart_userbot$")
        )
        
        # å¯¹è¯åˆ—è¡¨å‘½ä»¤
        self.client.add_event_handler(
            self.get_dialogs_command,
            events.NewMessage(pattern=r"^/get_dialogs$")
        )

        # æ–°å¢ï¼šå¤„ç†æ™®é€šæ–‡æœ¬æ¶ˆæ¯ä½œä¸ºæœç´¢ï¼ˆåº”åœ¨æ‰€æœ‰ç‰¹å®šå‘½ä»¤ä¹‹åæ³¨å†Œï¼‰
        self.client.add_event_handler(
            self.handle_plain_text_message,
            events.NewMessage(func=self._is_plain_text_and_not_command)
        )

        # Search Cache Admin Commands
        self.client.add_event_handler(
            self.view_search_config_command,
            events.NewMessage(pattern=r"^/view_search_config$")
        )
        self.client.add_event_handler(
            self.set_search_config_command,
            events.NewMessage(pattern=r"^/set_search_config(?:\s+(\S+))?(?:\s+(.+))?$")
        )
        self.client.add_event_handler(
            self.clear_search_cache_command,
            events.NewMessage(pattern=r"^/clear_search_cache$")
        )
        
        # Dialogs Cache Admin Commands
        self.client.add_event_handler(
            self.view_dialogs_cache_command,
            events.NewMessage(pattern=r"^/view_dialogs_cache$")
        )
        self.client.add_event_handler(
            self.clear_dialogs_cache_command,
            events.NewMessage(pattern=r"^/clear_dialogs_cache$")
        )
        
        # æœ€æ—§åŒæ­¥æ—¶é—´è®¾ç½®å‘½ä»¤
        self.client.add_event_handler(
            self.set_oldest_sync_time_command,
            events.NewMessage(pattern=r"^/set_oldest_sync_time(?:\s+(-?\d+))?(?:\s+(.+))?$")
        )
        self.client.add_event_handler(
            self.view_oldest_sync_time_command,
            events.NewMessage(pattern=r"^/view_oldest_sync_time(?:\s+(-?\d+))?$")
        )
        
        logger.info("å·²æ³¨å†Œæ‰€æœ‰å‘½ä»¤å¤„ç†å‡½æ•°ï¼ŒåŒ…æ‹¬æ™®é€šæ–‡æœ¬æœç´¢å¤„ç†å™¨ã€æœç´¢ç¼“å­˜å’Œå¯¹è¯ç¼“å­˜ç®¡ç†å‘½ä»¤")
    
    def _is_plain_text_and_not_command(self, event) -> bool:
        """
        æ£€æŸ¥æ¶ˆæ¯æ˜¯å¦ä¸ºæ™®é€šæ–‡æœ¬æ¶ˆæ¯ï¼Œå¹¶ä¸”ä¸æ˜¯ä¸€ä¸ªå·²çŸ¥çš„å‘½ä»¤ã€‚
        åªå¤„ç†æ¥è‡ªç”¨æˆ·çš„æ¶ˆæ¯ï¼Œå¿½ç•¥é¢‘é“å¹¿æ’­ç­‰ã€‚
        """
        # ç¡®ä¿æ¶ˆæ¯æ¥è‡ªç”¨æˆ· (ä¸æ˜¯é¢‘é“è‡ªåŠ¨å‘å¸ƒç­‰)
        if not event.is_private and not event.is_group: # ç®€å•åˆ¤æ–­ï¼Œå¯æ ¹æ®éœ€æ±‚è°ƒæ•´
             if event.chat and hasattr(event.chat, 'broadcast') and event.chat.broadcast:
                 return False # æ˜¯é¢‘é“å¹¿æ’­

        if not event.message or not event.message.text:
            return False # æ²¡æœ‰æ–‡æœ¬å†…å®¹ (ä¾‹å¦‚å›¾ç‰‡ã€è´´çº¸)
        
        text = event.message.text.strip()
        if not text: # æ¶ˆæ¯ä¸ºç©ºæˆ–åªæœ‰ç©ºæ ¼
            return False

        # æ£€æŸ¥æ˜¯å¦ä»¥å·²çŸ¥å‘½ä»¤å‰ç¼€å¼€å¤´
        # æ³¨æ„ï¼šè¿™é‡Œçš„å‘½ä»¤åˆ—è¡¨åº”è¯¥ä¸ register_handlers ä¸­æ³¨å†Œçš„å‘½ä»¤ä¿æŒä¸€è‡´
        known_commands_patterns = [
            r"^/start$",
            r"^/help$",
            r"^/search(?:\s+(.+))?$",
            r"^/add_whitelist(?:\s+(-?\d+))?$",
            r"^/remove_whitelist(?:\s+(-?\d+))?$",
            r"^/set_userbot_config(?:\s+(\S+))?(?:\s+(.+))?$",
            r"^/view_userbot_config$",
            r"^/restart_userbot$",
            r"^/get_dialogs$",
            # Add new search config commands to prevent them being treated as plain text
            r"^/view_search_config$",
            r"^/set_search_config(?:\s+(\S+))?(?:\s+(.+))?$",
            r"^/clear_search_cache$",
            r"^/view_dialogs_cache$",
            r"^/clear_dialogs_cache$"
        ]
        
        for pattern in known_commands_patterns:
            if re.match(pattern, text):
                return False # åŒ¹é…å·²çŸ¥å‘½ä»¤æ ¼å¼

        # è¿›ä¸€æ­¥æ’é™¤ä»»ä½•ä»¥ / å¼€å¤´çš„æ¶ˆæ¯ï¼Œä»¥é˜²æœ‰æœªæ˜ç¡®åˆ—å‡ºçš„å‘½ä»¤
        if text.startswith('/'):
            return False
            
        # æ–°å¢å‘½ä»¤
        r"^/set_oldest_sync_time(?:\s+(-?\d+))?(?:\s+(.+))?$",
        r"^/view_oldest_sync_time(?:\s+(-?\d+))?$",
            
        return True # æ˜¯æ™®é€šæ–‡æœ¬æ¶ˆæ¯ï¼Œä¸”ä¸æ˜¯å·²çŸ¥å‘½ä»¤

    async def handle_plain_text_message(self, event) -> None:
        """
        å¤„ç†æ™®é€šæ–‡æœ¬æ¶ˆæ¯ï¼Œå°†å…¶ä½œä¸ºæœç´¢æŸ¥è¯¢ã€‚
        """
        query = event.message.text.strip()
        # ç¡®ä¿æŸ¥è¯¢ä¸ä¸ºç©ºï¼ˆè™½ç„¶ _is_plain_text_and_not_command å·²ç»æ£€æŸ¥è¿‡ï¼‰
        if not query:
            return

        logger.info(f"æ¥æ”¶åˆ°æ™®é€šæ–‡æœ¬æ¶ˆæ¯ï¼Œå°†ä½œä¸ºæœç´¢æŸ¥è¯¢: '{query}' from user {(await event.get_sender()).id}")
        await self._perform_search(event, query, is_direct_search=True)

    async def is_admin(self, event) -> bool:
        """
        æ£€æŸ¥ç”¨æˆ·æ˜¯å¦ä¸ºç®¡ç†å‘˜
        
        Args:
            event: Telethon äº‹ä»¶å¯¹è±¡
            
        Returns:
            bool: ç”¨æˆ·æ˜¯å¦ä¸ºç®¡ç†å‘˜
        """
        sender = await event.get_sender()
        user_id = sender.id
        
        is_admin = user_id in self.admin_ids
        if not is_admin:
            logger.warning(f"ç”¨æˆ· {user_id} å°è¯•æ‰§è¡Œç®¡ç†å‘˜å‘½ä»¤ï¼Œä½†ä¸åœ¨ç®¡ç†å‘˜åˆ—è¡¨ä¸­")
        
        return is_admin
    
    async def start_command(self, event) -> None:
        """
        å¤„ç† /start å‘½ä»¤
        
        å‘é€æ¬¢è¿æ¶ˆæ¯å’ŒåŸºæœ¬ä½¿ç”¨è¯´æ˜
        
        Args:
            event: Telethon äº‹ä»¶å¯¹è±¡
        """
        try:
            sender = await event.get_sender()
            user_name = getattr(sender, 'first_name', 'User')
            
            # æ„å»ºæ¬¢è¿æ¶ˆæ¯
            welcome_message = (
                f"ğŸ‘‹ **ä½ å¥½ï¼Œ{user_name}ï¼**\n\n"
                f"æ¬¢è¿ä½¿ç”¨ Telegram ä¸­æ–‡å†å²æ¶ˆæ¯æœç´¢æœºå™¨äººã€‚\n\n"
                f"ä½ å¯ä»¥é€šè¿‡å‘é€ `/search å…³é”®è¯` æ¥æœç´¢å†å²æ¶ˆæ¯ã€‚\n"
                f"ä¾‹å¦‚ï¼š`/search Python æ•™ç¨‹`\n\n"
                f"å‘é€ `/help` è·å–æ›´è¯¦ç»†çš„ä½¿ç”¨è¯´æ˜å’Œé«˜çº§æœç´¢è¯­æ³•ã€‚"
            )
            
            await event.respond(welcome_message, parse_mode='md') # å¯ç”¨ Markdown
            logger.info(f"å·²å‘é€æ¬¢è¿æ¶ˆæ¯ç»™ç”¨æˆ· {sender.id}")
            
        except Exception as e:
            logger.error(f"å¤„ç† /start å‘½ä»¤æ—¶å‡ºé”™: {e}")
            await event.respond("ğŸ˜• å¯åŠ¨æœºå™¨äººæ—¶å‡ºç°é”™è¯¯ï¼Œè¯·ç¨åå†è¯•ã€‚")
    
    async def help_command(self, event) -> None:
        """
        å¤„ç† /help å‘½ä»¤
        
        å‘é€è¯¦ç»†çš„å¸®åŠ©ä¿¡æ¯ï¼ŒåŒ…æ‹¬æœç´¢è¯­æ³•
        
        Args:
            event: Telethon äº‹ä»¶å¯¹è±¡
        """
        try:
            help_message = format_help_message()
            await event.respond(help_message, parse_mode='md') # å¯ç”¨ Markdown
            logger.info(f"å·²å‘é€å¸®åŠ©æ¶ˆæ¯ç»™ç”¨æˆ· {(await event.get_sender()).id}")
            
        except Exception as e:
            logger.error(f"å¤„ç† /help å‘½ä»¤æ—¶å‡ºé”™: {e}")
            await event.respond("ğŸ˜• è·å–å¸®åŠ©ä¿¡æ¯æ—¶å‡ºç°é”™è¯¯ï¼Œè¯·ç¨åå†è¯•ã€‚")

    async def _get_results_from_meili(self,
                                      parsed_query: str,
                                      filters: Optional[str],
                                      sort: List[str],
                                      page: int,
                                      hits_per_page: int,
                                      start_timestamp: Optional[int] = None,
                                      end_timestamp: Optional[int] = None,
                                      chat_types: Optional[List[str]] = None) -> Dict[str, Any]:
        """
        Helper function to call MeiliSearch and return results.
        
        Args:
            parsed_query: è§£æåçš„æŸ¥è¯¢å­—ç¬¦ä¸²
            filters: Meilisearch è¿‡æ»¤æ¡ä»¶å­—ç¬¦ä¸²
            sort: æ’åºè§„åˆ™åˆ—è¡¨
            page: é¡µç 
            hits_per_page: æ¯é¡µç»“æœæ•°
            start_timestamp: å¼€å§‹æ—¶é—´æˆ³ï¼ˆå¯é€‰ï¼‰
            end_timestamp: ç»“æŸæ—¶é—´æˆ³ï¼ˆå¯é€‰ï¼‰
            chat_types: èŠå¤©ç±»å‹åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            Dict[str, Any]: Meilisearch æœç´¢ç»“æœ
        """
        return self.meilisearch_service.search(
            query=parsed_query,
            filters=filters,
            sort=sort,
            page=page,
            hits_per_page=hits_per_page,
            start_timestamp=start_timestamp,
            end_timestamp=end_timestamp,
            chat_types=chat_types
        )

    async def _fetch_all_results_async(self, cache_key: str, parsed_query: str, filters_dict: Optional[Dict[str, Any]], meili_filters: Optional[str], sort_options: List[str], total_hits_estimate: int):
        """
        å¼‚æ­¥è·å–æ‰€æœ‰æœç´¢ç»“æœå¹¶æ›´æ–°ç¼“å­˜ã€‚
        """
        try:
            logger.info(f"åå°ä»»åŠ¡å¼€å§‹: ä¸º key='{cache_key}' è·å–å…¨éƒ¨ {total_hits_estimate} æ¡ç»“æœã€‚")
            # Fetch all results. MeiliSearch's limit parameter can be high.
            # We assume meilisearch_service.search handles pagination if we ask for all hits.
            # Let's fetch all in one go if total_hits_estimate is manageable, e.g., < 1000.
            # Otherwise, we might need a loop here, but for now, try to get all.
            # The MeiliSearch Python SDK's `search` method with `hits_per_page` set to total_hits
            # should ideally return all results if the server limit allows.
            # If total_hits_estimate is very large, this might be slow or hit server limits.
            # For now, we'll request all items.
            
            # A more robust approach for very large result sets would be to fetch in chunks
            # and append, but that complicates cache updates significantly.
            # Let's assume total_hits_estimate is within a reasonable limit for a single fetch.
            # The default limit for MeiliSearch is 1000 per query if not specified otherwise by `hitsPerPage`.
            
            all_results_data = []
            # If total_hits_estimate is 0, no need to fetch
            if total_hits_estimate == 0:
                logger.info(f"åå°ä»»åŠ¡: key='{cache_key}', æ€»å‘½ä¸­æ•°ä¸º0ï¼Œæ— éœ€è·å–ã€‚")
                self.cache_service.update_cache_to_complete(parsed_query, filters_dict, [], 0)
                return

            # Fetch all results. The MeiliSearch client handles pagination internally if we set a high limit.
            # We'll fetch all results in one go.
            # The `search` method in MeiliSearchService already handles `page` and `hits_per_page`.
            # To get all, we can set hits_per_page to total_hits and page to 1.
            # However, MeiliSearch has a default limit (often 1000).
            # For simplicity, we'll fetch up to a practical limit (e.g. 1000) for the "full" cache.
            # If total_hits > 1000, the "full" cache will contain the first 1000.
            # This is a pragmatic choice to avoid excessive memory/network for huge result sets.
            # The user can still paginate through MeiliSearch directly via callbacks if needed beyond this.
            
            fetch_limit = min(total_hits_estimate, 1000) # Cap full fetch at 1000 for now

            if fetch_limit > 0:
                # å‡†å¤‡ç­›é€‰å‚æ•°
                start_timestamp = None
                end_timestamp = None
                chat_types = None
                
                if filters_dict:
                    # æå–æ—¥æœŸèŒƒå›´
                    if 'date_range' in filters_dict:
                        date_range = filters_dict['date_range']
                        start_timestamp = date_range.get('start')
                        end_timestamp = date_range.get('end')
                    
                    # æå–èŠå¤©ç±»å‹
                    if 'chat_type' in filters_dict:
                        chat_types = filters_dict['chat_type']
                
                # è·å–æ‰€æœ‰ç»“æœ
                full_search_results_obj = await self._get_results_from_meili(
                    parsed_query=parsed_query,
                    filters=meili_filters,
                    sort=sort_options,
                    page=1, # Get all from the first page
                    hits_per_page=fetch_limit, # Request all (up to the cap)
                    start_timestamp=start_timestamp,
                    end_timestamp=end_timestamp,
                    chat_types=chat_types
                )
                all_results_data = full_search_results_obj.get('hits', [])
            
            self.cache_service.update_cache_to_complete(parsed_query, filters_dict, all_results_data, total_hits_estimate)
            logger.info(f"åå°ä»»åŠ¡å®Œæˆ: key='{cache_key}' çš„ç¼“å­˜å·²æ›´æ–°ä¸º {len(all_results_data)} æ¡å®Œæ•´ç»“æœ (æ€»é¢„ä¼° {total_hits_estimate})ã€‚")

        except Exception as e:
            logger.error(f"åå°ä»»åŠ¡å¤±è´¥: key='{cache_key}' è·å–å…¨éƒ¨ç»“æœæ—¶å‡ºé”™: {e}", exc_info=True)
            # Optionally, remove the partial cache entry or mark it as failed?
            # For now, the partial entry will remain until TTL.
        finally:
            if cache_key in self.active_full_fetches:
                del self.active_full_fetches[cache_key]

    async def _perform_search(self, event, query: str, page: int = 1, is_direct_search: bool = False) -> None:
        """
        æ‰§è¡Œæœç´¢æ“ä½œå¹¶å›å¤ç»“æœã€‚é›†æˆäº†ç¼“å­˜é€»è¾‘ã€‚
        
        Args:
            event: Telethon äº‹ä»¶å¯¹è±¡ã€‚
            query: æœç´¢å…³é”®è¯ã€‚
            page: è¯·æ±‚çš„é¡µç  (ç”¨äºåˆ†é¡µ)ã€‚
            is_direct_search: æ˜¯å¦ä¸ºç›´æ¥æ— å‘½ä»¤æœç´¢ã€‚
        """
        try:
            sender_id = (await event.get_sender()).id
            logger.info(f"ç”¨æˆ· {sender_id} æœç´¢: '{query}', é¡µç : {page}")

            parsed_query, filters_dict = self._parse_advanced_syntax(query)
            meili_filters = self._build_meilisearch_filters(filters_dict) if filters_dict else None
            
            hits_per_page = 5  # Standard items per page for display
            sort_options = ["date:desc"] # Default sort

            cached_data_entry = None
            if self.cache_service.is_cache_enabled():
                cached_data_entry = self.cache_service.get_from_cache(parsed_query, filters_dict)

            if cached_data_entry:
                cached_results, is_partial, total_hits_from_cache, fetch_ts = cached_data_entry
                logger.info(f"ç¼“å­˜å‘½ä¸­ for query '{parsed_query}'. Partial: {is_partial}, Total Hits: {total_hits_from_cache}")

                if not is_partial: # Full data in cache
                    results_to_format = {
                        'hits': cached_results[ (page - 1) * hits_per_page : page * hits_per_page ],
                        'query': parsed_query,
                        'processingTimeMs': 0, # From cache
                        'estimatedTotalHits': total_hits_from_cache
                    }
                    total_pages = (total_hits_from_cache + hits_per_page - 1) // hits_per_page if total_hits_from_cache > 0 else 0
                    formatted_message, buttons = format_search_results(results_to_format, page, total_pages, query_original=query)
                    await event.respond(formatted_message, buttons=buttons, parse_mode='md')
                    logger.info(f"å·²ä»å®Œæ•´ç¼“å­˜å‘ç”¨æˆ· {sender_id} å‘é€ç¬¬ {page} é¡µç»“æœ")
                    return

                # Partial data in cache
                if total_hits_from_cache is not None:
                    # Check if requested page is within the initial fetch
                    if page * hits_per_page <= len(cached_results):
                        results_to_format = {
                            'hits': cached_results[ (page - 1) * hits_per_page : page * hits_per_page ],
                            'query': parsed_query,
                            'processingTimeMs': 0, # From cache
                            'estimatedTotalHits': total_hits_from_cache
                        }
                        total_pages = (total_hits_from_cache + hits_per_page - 1) // hits_per_page if total_hits_from_cache > 0 else 0
                        formatted_message, buttons = format_search_results(results_to_format, page, total_pages, query_original=query)
                        await event.respond(formatted_message, buttons=buttons, parse_mode='md')
                        logger.info(f"å·²ä»éƒ¨åˆ†ç¼“å­˜ (åˆå§‹è·å–éƒ¨åˆ†) å‘ç”¨æˆ· {sender_id} å‘é€ç¬¬ {page} é¡µç»“æœ")
                        return
                    else:
                        # Requested page is beyond initial fetch.
                        # Check if full fetch is ongoing or completed.
                        cache_key_for_async = self.cache_service._generate_cache_key(parsed_query, filters_dict) # pylint: disable=protected-access
                        if cache_key_for_async in self.active_full_fetches or (fetch_ts is not None and time.time() - fetch_ts < 60): # Task running or recently started (give it 60s)
                            # Chosen:æ–¹æ¡ˆA (æç¤ºç”¨æˆ·ç­‰å¾…) for pagination beyond initial while async fetch is running
                            await event.respond("â³ æ­£åœ¨åŠ è½½æ›´å¤šç»“æœï¼Œè¯·ç¨å€™ç‰‡åˆ»å†å°è¯•ç¿»é¡µ...", parse_mode='md')
                            logger.info(f"ç”¨æˆ· {sender_id} è¯·æ±‚çš„é¡µé¢è¶…å‡ºåˆå§‹ç¼“å­˜ï¼Œåå°ä»»åŠ¡ä»åœ¨è¿›è¡Œä¸­ã€‚")
                            # Record this choice in activeContext.md
                            # Decision: For pagination requests beyond the initial cached set, while a background
                            # full-fetch task is active (or was very recently initiated), we will inform the user
                            # that more results are loading and ask them to wait. This avoids hitting MeiliSearch
                            # again for a page that will soon be part of the complete cache entry, and provides
                            # a better UX than an immediate (potentially partial or inconsistent) result.
                            # The alternative of fetching this specific page directly from MeiliSearch might lead
                            # to this page not being part of the "all_results_data" when the background task completes,
                            # or requiring complex merging logic.
                            return
                        else:
                            # Full fetch might have completed and updated the cache, or failed/timed out.
                            # Re-check cache, it might be complete now.
                            fresh_cached_entry = self.cache_service.get_from_cache(parsed_query, filters_dict)
                            if fresh_cached_entry and not fresh_cached_entry[1]: # is_partial is False
                                cached_results_updated, _, total_hits_updated, _ = fresh_cached_entry
                                results_to_format = {
                                    'hits': cached_results_updated[ (page - 1) * hits_per_page : page * hits_per_page ],
                                    'query': parsed_query,
                                    'processingTimeMs': 0,
                                    'estimatedTotalHits': total_hits_updated
                                }
                                total_pages = (total_hits_updated + hits_per_page - 1) // hits_per_page if total_hits_updated > 0 else 0
                                formatted_message, buttons = format_search_results(results_to_format, page, total_pages, query_original=query)
                                await event.respond(formatted_message, buttons=buttons, parse_mode='md')
                                logger.info(f"å·²ä»æ›´æ–°åçš„å®Œæ•´ç¼“å­˜å‘ç”¨æˆ· {sender_id} å‘é€ç¬¬ {page} é¡µç»“æœ")
                                return
                            # If still partial or not found, proceed to fetch from Meili (should ideally not happen if logic is correct)
                            logger.warning(f"ç¼“å­˜çŠ¶æ€å¼‚å¸¸ for {parsed_query} after async check, proceeding to MeiliSearch for page {page}")


            # Cache miss or only partial data that doesn't cover the page and async fetch not active/helpful
            # This part is primarily for the first time a search is made (page=1)
            if page == 1: # Only do initial + async fetch on the first page request
                status_message = await event.respond("ğŸ” æ­£åœ¨æœç´¢ï¼Œè¯·ç¨å€™...", parse_mode='md')
                
                initial_fetch_count = self.cache_service.get_initial_fetch_count()
                
                # å‡†å¤‡ç­›é€‰å‚æ•°
                start_timestamp = None
                end_timestamp = None
                chat_types = None
                
                if filters_dict:
                    # æå–æ—¥æœŸèŒƒå›´
                    if 'date_range' in filters_dict:
                        date_range = filters_dict['date_range']
                        start_timestamp = date_range.get('start')
                        end_timestamp = date_range.get('end')
                    
                    # æå–èŠå¤©ç±»å‹
                    if 'chat_type' in filters_dict:
                        chat_types = filters_dict['chat_type']
                
                # Stage 1: Initial Fetch
                initial_results_obj = await self._get_results_from_meili(
                    parsed_query, meili_filters, sort_options, 1, initial_fetch_count,
                    start_timestamp=start_timestamp, end_timestamp=end_timestamp, chat_types=chat_types
                )
                initial_hits_data = initial_results_obj.get('hits', [])
                estimated_total_hits = initial_results_obj.get('estimatedTotalHits', 0)

                # Store initial results in cache
                full_fetch_ts = None
                if self.cache_service.is_cache_enabled():
                    if estimated_total_hits > len(initial_hits_data):
                        full_fetch_ts = time.time() # Mark time if async fetch will be needed
                    self.cache_service.store_in_cache(
                        parsed_query, filters_dict, initial_hits_data, estimated_total_hits,
                        is_partial=(estimated_total_hits > len(initial_hits_data)),
                        full_fetch_initiated_timestamp=full_fetch_ts
                    )

                # Format and send initial results
                total_pages_for_initial = (estimated_total_hits + hits_per_page - 1) // hits_per_page if estimated_total_hits > 0 else 0
                formatted_message, buttons = format_search_results(initial_results_obj, 1, total_pages_for_initial, query_original=query)
                
                try:
                    await status_message.edit(formatted_message, buttons=buttons, parse_mode='md')
                except Exception: # If edit fails (e.g. message too old)
                    await event.respond(formatted_message, buttons=buttons, parse_mode='md')
                logger.info(f"å·²å‘ç”¨æˆ· {sender_id} å‘é€åˆå§‹ {len(initial_hits_data)} æ¡æœç´¢ç»“æœ (æ€»å…± {estimated_total_hits} æ¡)")

                # Stage 2: Asynchronous Full Fetch (if needed)
                if self.cache_service.is_cache_enabled() and estimated_total_hits > len(initial_hits_data):
                    cache_key_for_async = self.cache_service._generate_cache_key(parsed_query, filters_dict) # pylint: disable=protected-access
                    if cache_key_for_async not in self.active_full_fetches:
                        logger.info(f"å¯åŠ¨åå°ä»»åŠ¡: ä¸º key='{cache_key_for_async}' è·å–å‰©ä½™ç»“æœã€‚")
                        task = asyncio.create_task(
                            self._fetch_all_results_async(
                                cache_key_for_async, parsed_query, filters_dict, meili_filters, sort_options, estimated_total_hits
                            )
                        )
                        self.active_full_fetches[cache_key_for_async] = task
                    else:
                        logger.info(f"åå°ä»»åŠ¡å·²åœ¨è¿è¡Œ: key='{cache_key_for_async}'")
                return # Initial results sent, async fetch (if any) started.

            else: # page > 1 and data not sufficiently in cache
                # This case means user is asking for a subsequent page,
                # but the cache (even after checking for async completion) doesn't have it.
                # This might happen if TTL expired, or cache was cleared, or async failed silently.
                # For robustness, fetch this specific page directly from MeiliSearch.
                # This page won't be part of the "full_fetch" logic if it runs later for the same query.
                logger.warning(f"ç¼“å­˜æœªå‘½ä¸­æˆ–æ•°æ®ä¸è¶³ (é¡µç  {page}) for '{parsed_query}'. ç›´æ¥ä» MeiliSearch è·å–ã€‚")
                status_message = await event.respond(f"ğŸ” æ­£åœ¨åŠ è½½ç¬¬ {page} é¡µï¼Œè¯·ç¨å€™...", parse_mode='md')
                
                # å‡†å¤‡ç­›é€‰å‚æ•°
                start_timestamp = None
                end_timestamp = None
                chat_types = None
                
                if filters_dict:
                    # æå–æ—¥æœŸèŒƒå›´
                    if 'date_range' in filters_dict:
                        date_range = filters_dict['date_range']
                        start_timestamp = date_range.get('start')
                        end_timestamp = date_range.get('end')
                    
                    # æå–èŠå¤©ç±»å‹
                    if 'chat_type' in filters_dict:
                        chat_types = filters_dict['chat_type']
                
                # è·å–ç‰¹å®šé¡µé¢çš„ç»“æœ
                page_specific_results_obj = await self._get_results_from_meili(
                    parsed_query, meili_filters, sort_options, page, hits_per_page,
                    start_timestamp=start_timestamp, end_timestamp=end_timestamp, chat_types=chat_types
                )
                estimated_total_hits = page_specific_results_obj.get('estimatedTotalHits', 0) # Re-confirm total
                total_pages = (estimated_total_hits + hits_per_page - 1) // hits_per_page if estimated_total_hits > 0 else 0
                
                formatted_message, buttons = format_search_results(page_specific_results_obj, page, total_pages, query_original=query)
                try:
                    await status_message.edit(formatted_message, buttons=buttons, parse_mode='md')
                except Exception:
                    await event.respond(formatted_message, buttons=buttons, parse_mode='md')
                logger.info(f"å·²ç›´æ¥ä» MeiliSearch å‘ç”¨æˆ· {sender_id} å‘é€ç¬¬ {page} é¡µç»“æœ")
                return

        except Exception as e:
            logger.error(f"æ‰§è¡Œæœç´¢æ—¶å‡ºé”™ (query: {query}, page: {page}): {e}", exc_info=True)
            error_message = format_error_message(str(e))
            # Try to edit if status_message exists, otherwise respond
            try:
                if 'status_message' in locals() and status_message:
                    await status_message.edit(error_message, parse_mode='md')
                else:
                    await event.respond(error_message, parse_mode='md')
            except Exception:
                 await event.respond(error_message, parse_mode='md')

    async def search_command(self, event) -> None:
        """
        å¤„ç† /search å‘½ä»¤
        
        æ‰§è¡Œæœç´¢å¹¶è¿”å›ç»“æœ
        
        Args:
            event: Telethon äº‹ä»¶å¯¹è±¡
        """
        message_text = event.message.text
        match = re.match(r"^/search(?:\s+(.+))?$", message_text)
        
        if not match or not match.group(1):
            await event.respond("è¯·æä¾›æœç´¢å…³é”®è¯ï¼Œä¾‹å¦‚ï¼š`/search Python æ•™ç¨‹`\nå‘é€ `/help` è·å–æ›´å¤šä½¿ç”¨è¯´æ˜ã€‚")
            return
        
        query = match.group(1).strip()
        # For /search command, it's always the first page initially
        await self._perform_search(event, query, page=1)
    
    def _parse_advanced_syntax(self, query: str) -> Tuple[str, Dict[str, Any]]:
        """
        è§£æé«˜çº§æœç´¢è¯­æ³•
        
        æ”¯æŒçš„è¯­æ³•:
        - ç²¾ç¡®çŸ­è¯­: "å…³é”®çŸ­è¯­"
        - ç±»å‹ç­›é€‰: type:ç±»å‹ (user/group/channel)
          å¯ä»¥å¤šæ¬¡ä½¿ç”¨æ­¤è¯­æ³•æ¥ç­›é€‰å¤šç§ç±»å‹ï¼Œå¦‚: type:group type:channel
        - æ—¶é—´ç­›é€‰: date:èµ·å§‹_ç»“æŸ (YYYY-MM-DD_YYYY-MM-DD)
        
        Args:
            query: åŸå§‹æŸ¥è¯¢å­—ç¬¦ä¸²
            
        Returns:
            Tuple[str, Dict[str, Any]]: å¤„ç†åçš„æŸ¥è¯¢å’Œè¿‡æ»¤æ¡ä»¶å­—å…¸
        """
        # åˆå§‹åŒ–ç»“æœ
        filters = {}
        
        # å¤„ç†ç±»å‹ç­›é€‰ (å¯èƒ½æœ‰å¤šä¸ª)
        chat_types = []
        type_matches = re.finditer(r'type:(\w+)', query)
        valid_chat_types = ['user', 'group', 'channel']
        
        for match in type_matches:
            chat_type = match.group(1).lower()
            if chat_type in valid_chat_types and chat_type not in chat_types:
                chat_types.append(chat_type)
        
        if chat_types:
            filters['chat_type'] = chat_types
            # ä»æŸ¥è¯¢ä¸­ç§»é™¤æ‰€æœ‰ type: éƒ¨åˆ†
            query = re.sub(r'type:\w+', '', query).strip()
        
        # å¤„ç†æ—¶é—´ç­›é€‰
        date_match = re.search(r'date:(\d{4}-\d{2}-\d{2})(?:_(\d{4}-\d{2}-\d{2}))?', query)
        if date_match:
            start_date = date_match.group(1)
            end_date = date_match.group(2) if date_match.group(2) else datetime.now().strftime('%Y-%m-%d')
            
            # è½¬æ¢ä¸º Unix æ—¶é—´æˆ³
            try:
                start_timestamp = int(datetime.strptime(start_date, '%Y-%m-%d').timestamp())
                # å¯¹äºç»“æŸæ—¥æœŸï¼Œè®¾ç½®ä¸ºå½“å¤©çš„æœ€åä¸€ç§’
                end_timestamp = int(datetime.strptime(end_date + ' 23:59:59', '%Y-%m-%d %H:%M:%S').timestamp())
                
                filters['date_range'] = {
                    'start': start_timestamp,
                    'end': end_timestamp
                }
                
                # ä»æŸ¥è¯¢ä¸­ç§»é™¤ date: éƒ¨åˆ†
                query = re.sub(r'date:\d{4}-\d{2}-\d{2}(?:_\d{4}-\d{2}-\d{2})?', '', query).strip()
            except ValueError as e:
                logger.warning(f"æ—¥æœŸè§£æé”™è¯¯: {e}")
        
        # æ¸…ç†æŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œç§»é™¤å¤šä½™ç©ºæ ¼
        clean_query = re.sub(r'\s+', ' ', query).strip()
        
        return clean_query, filters
    
    def _build_meilisearch_filters(self, filters_dict: Dict[str, Any]) -> str:
        """
        æ„å»º Meilisearch è¿‡æ»¤æ¡ä»¶å­—ç¬¦ä¸²
        
        Args:
            filters_dict: è¿‡æ»¤æ¡ä»¶å­—å…¸
            
        Returns:
            str: Meilisearch è¿‡æ»¤æ¡ä»¶å­—ç¬¦ä¸²
        """
        filter_parts = []
        
        # å¤„ç†èŠå¤©ç±»å‹è¿‡æ»¤
        if 'chat_type' in filters_dict:
            chat_type_value = filters_dict['chat_type']
            
            # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œæ„å»º OR æ¡ä»¶
            if isinstance(chat_type_value, list) and chat_type_value:
                chat_type_conditions = [f"chat_type = '{chat_type}'" for chat_type in chat_type_value]
                filter_parts.append(f"({' OR '.join(chat_type_conditions)})")
            # å¦‚æœæ˜¯å•ä¸ªå€¼ï¼Œç›´æ¥æ·»åŠ æ¡ä»¶
            elif isinstance(chat_type_value, str):
                filter_parts.append(f"chat_type = '{chat_type_value}'")
        
        # å¤„ç†æ—¥æœŸèŒƒå›´è¿‡æ»¤
        if 'date_range' in filters_dict:
            date_range = filters_dict['date_range']
            filter_parts.append(f"date >= {date_range['start']} AND date <= {date_range['end']}")
        
        # ç»„åˆæ‰€æœ‰è¿‡æ»¤æ¡ä»¶
        return ' AND '.join(filter_parts) if filter_parts else None
    
    async def add_whitelist_command(self, event) -> None:
        """
        å¤„ç† /add_whitelist å‘½ä»¤ (ç®¡ç†å‘˜æƒé™)
        
        æ·»åŠ èŠå¤©åˆ°ç™½åå•
        
        Args:
            event: Telethon äº‹ä»¶å¯¹è±¡
        """
        try:
            # æ£€æŸ¥æƒé™
            if not await self.is_admin(event):
                await event.respond("âš ï¸ æ­¤å‘½ä»¤éœ€è¦ç®¡ç†å‘˜æƒé™ã€‚")
                return
            
            # è·å– chat_id
            message_text = event.message.text
            match = re.match(r"^/add_whitelist(?:\s+(-?\d+))?$", message_text)
            
            if not match or not match.group(1):
                await event.respond("è¯·æä¾›è¦æ·»åŠ çš„ chat_idï¼Œä¾‹å¦‚ï¼š`/add_whitelist -1001234567890`")
                return
            
            chat_id = int(match.group(1))
            
            # æ·»åŠ åˆ°ç™½åå•
            success = self.config_manager.add_to_whitelist(chat_id)
            
            if success:
                await event.respond(f"âœ… å·²æˆåŠŸå°† chat_id `{chat_id}` æ·»åŠ åˆ°ç™½åå•ã€‚", parse_mode='md') # å¯ç”¨ Markdown
                logger.info(f"ç®¡ç†å‘˜ {(await event.get_sender()).id} æ·»åŠ  {chat_id} åˆ°ç™½åå•")
            else:
                await event.respond(f"â„¹ï¸ chat_id `{chat_id}` å·²åœ¨ç™½åå•ä¸­ï¼Œæ— éœ€é‡å¤æ·»åŠ ã€‚", parse_mode='md') # å¯ç”¨ Markdown
            
        except Exception as e:
            logger.error(f"å¤„ç† /add_whitelist å‘½ä»¤æ—¶å‡ºé”™: {e}")
            await event.respond(f"âš ï¸ æ·»åŠ ç™½åå•æ—¶å‡ºç°é”™è¯¯: {str(e)}")
    
    async def remove_whitelist_command(self, event) -> None:
        """
        å¤„ç† /remove_whitelist å‘½ä»¤ (ç®¡ç†å‘˜æƒé™)
        
        ä»ç™½åå•ç§»é™¤èŠå¤©
        
        Args:
            event: Telethon äº‹ä»¶å¯¹è±¡
        """
        try:
            # æ£€æŸ¥æƒé™
            if not await self.is_admin(event):
                await event.respond("âš ï¸ æ­¤å‘½ä»¤éœ€è¦ç®¡ç†å‘˜æƒé™ã€‚")
                return
            
            # è·å– chat_id
            message_text = event.message.text
            match = re.match(r"^/remove_whitelist(?:\s+(-?\d+))?$", message_text)
            
            if not match or not match.group(1):
                await event.respond("è¯·æä¾›è¦ç§»é™¤çš„ chat_idï¼Œä¾‹å¦‚ï¼š`/remove_whitelist -1001234567890`")
                return
            
            chat_id = int(match.group(1))
            
            # ä»ç™½åå•ç§»é™¤
            success = self.config_manager.remove_from_whitelist(chat_id)
            
            if success:
                await event.respond(f"âœ… å·²æˆåŠŸå°† chat_id `{chat_id}` ä»ç™½åå•ç§»é™¤ã€‚", parse_mode='md') # å¯ç”¨ Markdown
                logger.info(f"ç®¡ç†å‘˜ {(await event.get_sender()).id} ä»ç™½åå•ç§»é™¤ {chat_id}")
            else:
                await event.respond(f"â„¹ï¸ chat_id `{chat_id}` ä¸åœ¨ç™½åå•ä¸­ï¼Œæ— éœ€ç§»é™¤ã€‚", parse_mode='md') # å¯ç”¨ Markdown
            
        except Exception as e:
            logger.error(f"å¤„ç† /remove_whitelist å‘½ä»¤æ—¶å‡ºé”™: {e}")
            await event.respond(f"âš ï¸ ç§»é™¤ç™½åå•æ—¶å‡ºç°é”™è¯¯: {str(e)}")
            
    async def set_userbot_config_command(self, event) -> None:
        """
        å¤„ç† /set_userbot_config å‘½ä»¤ (ç®¡ç†å‘˜æƒé™)
        
        è®¾ç½® User Bot é…ç½®é¡¹
        
        Args:
            event: Telethon äº‹ä»¶å¯¹è±¡
        """
        try:
            # æ£€æŸ¥æƒé™
            if not await self.is_admin(event):
                await event.respond("âš ï¸ æ­¤å‘½ä»¤éœ€è¦ç®¡ç†å‘˜æƒé™ã€‚")
                return
            
            # è·å–å‚æ•°
            message_text = event.message.text
            match = re.match(r"^/set_userbot_config(?:\s+(\S+))?(?:\s+(.+))?$", message_text)
            
            if not match or not match.group(1):
                help_text = """è¯·æä¾›è¦è®¾ç½®çš„é…ç½®é¡¹å’Œå€¼ï¼Œä¾‹å¦‚ï¼š
`/set_userbot_config USER_SESSION_NAME my_session`

å¯è®¾ç½®çš„é…ç½®é¡¹åŒ…æ‹¬ï¼š
- `USER_API_ID` - Telegram API ID
- `USER_API_HASH` - Telegram API Hash
- `USER_SESSION_NAME` - ä¼šè¯åç§°ï¼ˆå¦‚éœ€ä¿®æ”¹ï¼Œéœ€è¦é‡å¯ User Botï¼‰
- `USER_PROXY_URL` - ä»£ç†æœåŠ¡å™¨ URLï¼ˆå¦‚éœ€ä½¿ç”¨ï¼‰

âš ï¸ æ³¨æ„ï¼šä¿®æ”¹é…ç½®åï¼Œéœ€è¦ä½¿ç”¨ `/restart_userbot` å‘½ä»¤ä½¿é…ç½®ç”Ÿæ•ˆã€‚"""
            await event.respond(help_text, parse_mode='md') # å¯ç”¨ Markdown
            return
            
            key = match.group(1).upper()  # è½¬ä¸ºå¤§å†™
            if not match.group(2):
                await event.respond(f"è¯·æä¾› `{key}` çš„å€¼ï¼Œä¾‹å¦‚ï¼š`/set_userbot_config {key} value`")
                return
                
            value = match.group(2).strip()
            
            # æ·»åŠ USER_å‰ç¼€ï¼ˆå¦‚æœæ²¡æœ‰ï¼‰
            if not key.startswith("USER_"):
                key = f"USER_{key}"
                
            # è®¾ç½®é…ç½®
            self.config_manager.set_userbot_env(key, value)
            
            # å‘é€æˆåŠŸæ¶ˆæ¯
            await event.respond(f"âœ… å·²è®¾ç½® User Bot é…ç½®é¡¹ `{key}` = `{value if key != 'USER_API_HASH' else '******'}`\n\nä½¿ç”¨ `/restart_userbot` å‘½ä»¤ä½¿é…ç½®ç”Ÿæ•ˆã€‚", parse_mode='md') # å¯ç”¨ Markdown
            logger.info(f"ç®¡ç†å‘˜ {(await event.get_sender()).id} è®¾ç½® User Bot é…ç½®é¡¹ {key}")
            
        except Exception as e:
            logger.error(f"å¤„ç† /set_userbot_config å‘½ä»¤æ—¶å‡ºé”™: {e}")
            await event.respond(f"âš ï¸ è®¾ç½® User Bot é…ç½®æ—¶å‡ºç°é”™è¯¯: {str(e)}")
            
    async def view_userbot_config_command(self, event) -> None:
        """
        å¤„ç† /view_userbot_config å‘½ä»¤ (ç®¡ç†å‘˜æƒé™)
        
        æŸ¥çœ‹ User Bot å½“å‰é…ç½®
        
        Args:
            event: Telethon äº‹ä»¶å¯¹è±¡
        """
        try:
            # æ£€æŸ¥æƒé™
            if not await self.is_admin(event):
                await event.respond("âš ï¸ æ­¤å‘½ä»¤éœ€è¦ç®¡ç†å‘˜æƒé™ã€‚")
                return
            
            # è·å–é…ç½®
            config_dict = self.config_manager.get_userbot_config_dict(exclude_sensitive=True)
            
            if not config_dict:
                await event.respond("â„¹ï¸ User Bot å°šæœªé…ç½®ä»»ä½•ç¯å¢ƒå˜é‡ã€‚ä½¿ç”¨ `/set_userbot_config` å‘½ä»¤è¿›è¡Œé…ç½®ã€‚")
                return
                
            # æ ¼å¼åŒ–é…ç½®ä¿¡æ¯
            config_text = "ğŸ“ **User Bot å½“å‰é…ç½®**\n\n"
            for key, value in config_dict.items():
                config_text += f"- `{key}` = `{value}`\n"
                
            config_text += "\nä½¿ç”¨ `/set_userbot_config <key> <value>` ä¿®æ”¹é…ç½®ï¼Œä½¿ç”¨ `/restart_userbot` ä½¿é…ç½®ç”Ÿæ•ˆã€‚"
            
            await event.respond(config_text, parse_mode='md') # å¯ç”¨ Markdown
            logger.info(f"ç®¡ç†å‘˜ {(await event.get_sender()).id} æŸ¥çœ‹ User Bot é…ç½®")
            
        except Exception as e:
            logger.error(f"å¤„ç† /view_userbot_config å‘½ä»¤æ—¶å‡ºé”™: {e}")
            await event.respond(f"âš ï¸ æŸ¥çœ‹ User Bot é…ç½®æ—¶å‡ºç°é”™è¯¯: {str(e)}")
            
    async def restart_userbot_command(self, event) -> None:
        """
        å¤„ç† /restart_userbot å‘½ä»¤ (ç®¡ç†å‘˜æƒé™)
        
        é‡å¯ User Bot
        
        Args:
            event: Telethon äº‹ä»¶å¯¹è±¡
        """
        try:
            # æ£€æŸ¥æƒé™
            if not await self.is_admin(event):
                await event.respond("âš ï¸ æ­¤å‘½ä»¤éœ€è¦ç®¡ç†å‘˜æƒé™ã€‚")
                return
            
            # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¯äº‹ä»¶
            if not self.userbot_restart_event:
                await event.respond("âš ï¸ é‡å¯åŠŸèƒ½æœªåˆå§‹åŒ–ï¼Œæ— æ³•é‡å¯ User Botã€‚")
                logger.error("å°è¯•é‡å¯ User Botï¼Œä½† userbot_restart_event æœªåˆå§‹åŒ–")
                return
                
            # å‘é€é‡å¯æ¶ˆæ¯
            await event.respond("ğŸ”„ æ­£åœ¨é‡å¯ User Botï¼Œè¯·ç¨å€™...")
            logger.info(f"ç®¡ç†å‘˜ {(await event.get_sender()).id} è§¦å‘ User Bot é‡å¯")
            
            # è®¾ç½®é‡å¯äº‹ä»¶
            self.userbot_restart_event.set()
            
            # ç­‰å¾…ä¸€æ®µæ—¶é—´ï¼Œè®©é‡å¯è¿‡ç¨‹å®Œæˆ
            await asyncio.sleep(5)
            
            # å‘é€é‡å¯å®Œæˆæ¶ˆæ¯
            await event.respond("âœ… User Bot å·²é‡æ–°å¯åŠ¨ï¼Œæ–°é…ç½®å·²ç”Ÿæ•ˆã€‚")
            
        except Exception as e:
            logger.error(f"å¤„ç† /restart_userbot å‘½ä»¤æ—¶å‡ºé”™: {e}")
            await event.respond(f"âš ï¸ é‡å¯ User Bot æ—¶å‡ºç°é”™è¯¯: {str(e)}")

    async def get_dialogs_command(self, event) -> None:
        """
        å¤„ç† /get_dialogs å‘½ä»¤
        
        è·å–ç”¨æˆ·è´¦æˆ·ä¸‹çš„æ‰€æœ‰å¯¹è¯åˆ—è¡¨ï¼ŒåŒ…æ‹¬å¯¹è¯åç§°å’ŒID
        æ”¯æŒ30åˆ†é’Ÿç¼“å­˜ä»¥æé«˜å“åº”é€Ÿåº¦
        
        Args:
            event: Telethon äº‹ä»¶å¯¹è±¡
        """
        try:
            sender = await event.get_sender()
            sender_id = sender.id
            logger.info(f"ç”¨æˆ· {sender_id} è¯·æ±‚è·å–å¯¹è¯åˆ—è¡¨")
            
            # é¦–å…ˆæ£€æŸ¥ç¼“å­˜
            cached_dialogs = None
            if self.dialogs_cache_service.is_cache_enabled():
                cached_dialogs = self.dialogs_cache_service.get_from_cache(sender_id)
            
            if cached_dialogs:
                # ç¼“å­˜å‘½ä¸­ï¼Œç›´æ¥ä½¿ç”¨ç¼“å­˜æ•°æ®
                logger.info(f"ç”¨æˆ· {sender_id} çš„å¯¹è¯åˆ—è¡¨ç¼“å­˜å‘½ä¸­ï¼Œå…± {len(cached_dialogs)} ä¸ªå¯¹è¯")
                all_dialogs_info = cached_dialogs
                
                # å‘é€ç¼“å­˜æç¤ºæ¶ˆæ¯ï¼ˆå¯é€‰ï¼‰
                status_message = await event.respond("ğŸ“‹ æ­£åœ¨åŠ è½½å¯¹è¯åˆ—è¡¨ï¼ˆæ¥è‡ªç¼“å­˜ï¼‰...")
            else:
                # ç¼“å­˜æœªå‘½ä¸­ï¼Œéœ€è¦ä»APIè·å–
                logger.info(f"ç”¨æˆ· {sender_id} çš„å¯¹è¯åˆ—è¡¨ç¼“å­˜æœªå‘½ä¸­ï¼Œä»APIè·å–")
                
                # å‘é€å¤„ç†ä¸­çš„æ¶ˆæ¯
                status_message = await event.respond("ğŸ” æ­£åœ¨è·å–å¯¹è¯åˆ—è¡¨ï¼Œè¯·ç¨å€™...")
                
                # è·å– UserBotClient å®ä¾‹
                try:
                    userbot_client = UserBotClient()
                    
                    # è°ƒç”¨è·å–å¯¹è¯ä¿¡æ¯çš„æ–¹æ³•
                    all_dialogs_info = await userbot_client.get_dialogs_info()
                    
                    # å°†ç»“æœå­˜å…¥ç¼“å­˜
                    if self.dialogs_cache_service.is_cache_enabled() and all_dialogs_info:
                        self.dialogs_cache_service.store_in_cache(sender_id, all_dialogs_info)
                        logger.info(f"ç”¨æˆ· {sender_id} çš„å¯¹è¯åˆ—è¡¨å·²å­˜å…¥ç¼“å­˜ï¼Œ30åˆ†é’Ÿå†…æœ‰æ•ˆ")
                    
                except RuntimeError as e:
                    # UserBot å®¢æˆ·ç«¯ç›¸å…³é”™è¯¯
                    error_msg = "âš ï¸ User Bot æœªæ­£ç¡®åˆå§‹åŒ–æˆ–æœªè¿æ¥ï¼Œæ— æ³•è·å–å¯¹è¯åˆ—è¡¨ã€‚\n\nè¯·è”ç³»ç®¡ç†å‘˜æ£€æŸ¥ User Bot çŠ¶æ€ã€‚"
                    await status_message.edit(error_msg, parse_mode='md')
                    logger.error(f"UserBot å®¢æˆ·ç«¯é”™è¯¯: {e}")
                    return
                    
                except Exception as e:
                    # å…¶ä»–é”™è¯¯
                    error_msg = f"âš ï¸ è·å–å¯¹è¯åˆ—è¡¨æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}\n\nè¯·ç¨åå†è¯•æˆ–è”ç³»ç®¡ç†å‘˜ã€‚"
                    await status_message.edit(error_msg, parse_mode='md')
                    logger.error(f"è·å–å¯¹è¯åˆ—è¡¨æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
                    return
            
            # æ£€æŸ¥å¯¹è¯åˆ—è¡¨æ˜¯å¦ä¸ºç©º
            if not all_dialogs_info:
                await status_message.edit("ğŸ“­ **å¯¹è¯åˆ—è¡¨ä¸ºç©º**\n\nå½“å‰è´¦æˆ·ä¸‹æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å¯¹è¯ã€‚", parse_mode='md')
                logger.info(f"ç”¨æˆ· {sender_id} çš„å¯¹è¯åˆ—è¡¨ä¸ºç©º")
                return

            # åˆ†é¡µè®¾ç½®
            dialogs_per_page = 15  # æ¯é¡µæ˜¾ç¤ºçš„å¯¹è¯æ•°é‡ï¼Œå¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´
            current_page = 1 # åˆå§‹è¯·æ±‚æ€»æ˜¯ç¬¬ä¸€é¡µ
            total_dialogs = len(all_dialogs_info)
            total_pages = (total_dialogs + dialogs_per_page - 1) // dialogs_per_page
            if total_pages == 0: # Handle case with 0 dialogs, though caught by `if not all_dialogs_info`
                total_pages = 1

            # æ ¼å¼åŒ–å¯¹è¯åˆ—è¡¨ï¼ˆç¬¬ä¸€é¡µï¼‰
            formatted_message, buttons = format_dialogs_list(
                dialogs_info=all_dialogs_info,
                current_page=current_page,
                total_pages=total_pages,
                items_per_page=dialogs_per_page
            )
            
            # æ›´æ–°æ¶ˆæ¯
            await status_message.edit(formatted_message, buttons=buttons, parse_mode='md')
            
            # è®°å½•æ—¥å¿—
            cache_status = "ï¼ˆæ¥è‡ªç¼“å­˜ï¼‰" if cached_dialogs else "ï¼ˆä»APIè·å–ï¼‰"
            logger.info(f"å·²å‘ç”¨æˆ· {sender_id} å‘é€å¯¹è¯åˆ—è¡¨ç¬¬ {current_page}/{total_pages} é¡µï¼Œå…± {total_dialogs} ä¸ªå¯¹è¯ {cache_status}")
                
        except Exception as e:
            logger.error(f"å¤„ç† /get_dialogs å‘½ä»¤æ—¶å‡ºé”™: {e}", exc_info=True)
            try:
                await event.respond("ğŸ˜• å¤„ç†è·å–å¯¹è¯åˆ—è¡¨è¯·æ±‚æ—¶å‡ºç°é”™è¯¯ï¼Œè¯·ç¨åå†è¯•ã€‚")
            except Exception:
                # å¦‚æœè¿å›å¤éƒ½å¤±è´¥äº†ï¼Œåªèƒ½è®°å½•æ—¥å¿—
                logger.error("æ— æ³•å‘é€é”™è¯¯å›å¤æ¶ˆæ¯")

    async def view_search_config_command(self, event) -> None:
        """å¤„ç† /view_search_config å‘½ä»¤ (ç®¡ç†å‘˜æƒé™)"""
        if not await self.is_admin(event):
            await event.respond("âš ï¸ æ­¤å‘½ä»¤éœ€è¦ç®¡ç†å‘˜æƒé™ã€‚")
            return
        try:
            config_text = "ğŸ” **æœç´¢ç¼“å­˜é…ç½®**\n\n"
            config_text += f"- å¯ç”¨ç¼“å­˜: `{self.config_manager.get_search_cache_enabled()}`\n"
            config_text += f"- ç¼“å­˜TTL (ç§’): `{self.config_manager.get_search_cache_ttl()}`\n"
            config_text += f"- åˆå§‹è·å–æ¡ç›®æ•°: `{self.config_manager.get_search_cache_initial_fetch_count()}`\n\n"
            
            cache_stats = self.cache_service.get_cache_stats()
            if cache_stats.get("enabled"):
                config_text += "**ç¼“å­˜çŠ¶æ€:**\n"
                config_text += f"- å½“å‰æ¡ç›®æ•°: `{cache_stats.get('currsize', 'N/A')}`\n"
                config_text += f"- æœ€å¤§æ¡ç›®æ•°: `{cache_stats.get('maxsize', 'N/A')}`\n"
            else:
                config_text += "**ç¼“å­˜çŠ¶æ€:** `å·²ç¦ç”¨`\n"
            
            await event.respond(config_text, parse_mode='md')
            logger.info(f"ç®¡ç†å‘˜ {(await event.get_sender()).id} æŸ¥çœ‹æœç´¢ç¼“å­˜é…ç½®")
        except Exception as e:
            logger.error(f"å¤„ç† /view_search_config å‘½ä»¤æ—¶å‡ºé”™: {e}")
            await event.respond(f"âš ï¸ æŸ¥çœ‹æœç´¢ç¼“å­˜é…ç½®æ—¶å‡ºç°é”™è¯¯: {str(e)}")

    async def set_search_config_command(self, event) -> None:
        """å¤„ç† /set_search_config å‘½ä»¤ (ç®¡ç†å‘˜æƒé™)"""
        if not await self.is_admin(event):
            await event.respond("âš ï¸ æ­¤å‘½ä»¤éœ€è¦ç®¡ç†å‘˜æƒé™ã€‚")
            return

        message_text = event.message.text
        match = re.match(r"^/set_search_config(?:\s+(\S+))?(?:\s+(.+))?$", message_text)

        if not match or not match.group(1) or not match.group(2):
            help_text = """è¯·æä¾›é…ç½®é¡¹å’Œå€¼ï¼Œä¾‹å¦‚ï¼š
`/set_search_config enable_search_cache true`

å¯è®¾ç½®çš„é…ç½®é¡¹:
- `enable_search_cache` (true/false)
- `search_cache_ttl_seconds` (æ•´æ•°, ä¾‹å¦‚ 3600)
- `search_cache_initial_fetch_count` (æ•´æ•°, ä¾‹å¦‚ 20)

æ›´æ”¹é…ç½®åï¼Œç¼“å­˜å°†é‡æ–°åˆå§‹åŒ–ã€‚"""
            await event.respond(help_text, parse_mode='md')
            return

        key = match.group(1).lower()
        value_str = match.group(2).strip()
        
        valid_keys = {
            "enable_search_cache": lambda v: v.lower() == 'true' or v.lower() == 'false',
            "search_cache_ttl_seconds": lambda v: v.isdigit(),
            "search_cache_initial_fetch_count": lambda v: v.isdigit()
        }

        if key not in valid_keys:
            await event.respond(f"âš ï¸ æ— æ•ˆçš„é…ç½®é¡¹: `{key}`ã€‚è¯·ä»å…è®¸çš„åˆ—è¡¨ä¸­é€‰æ‹©ã€‚")
            return

        try:
            processed_value: Union[bool, int, str]
            if key == "enable_search_cache":
                if value_str.lower() not in ['true', 'false']:
                    raise ValueError("enable_search_cache å¿…é¡»æ˜¯ true æˆ– false")
                processed_value = value_str.lower() == 'true'
            elif key in ["search_cache_ttl_seconds", "search_cache_initial_fetch_count"]:
                if not value_str.isdigit():
                    raise ValueError(f"{key} å¿…é¡»æ˜¯ä¸€ä¸ªæ•´æ•°")
                processed_value = int(value_str)
                if processed_value <= 0 and key != "search_cache_ttl_seconds": # TTL can be 0 for no expiry with maxsize
                     if processed_value <=0 and key == "search_cache_ttl_seconds" and self.config_manager.config.getint("SearchBot", "search_cache_ttl_seconds", fallback=1) == 0 : # allow 0 if already 0 (no expiry)
                         pass # allow 0 for TTL if it means no expiry based on cachetools
                     elif key == "search_cache_initial_fetch_count" and processed_value <=0:
                         raise ValueError(f"{key} å¿…é¡»å¤§äº 0")

            else: # Should not happen due to key check
                await event.respond(f"âš ï¸ æœªçŸ¥çš„é…ç½®é”®: {key}")
                return

            # Update ConfigParser in memory
            section = "SearchBot"
            if not self.config_manager.config.has_section(section):
                self.config_manager.config.add_section(section)
            
            self.config_manager.config.set(section, key, str(processed_value)) # Store as string in ini

            # Save to config.ini
            with open(self.config_manager.config_path, "w", encoding="utf-8") as f:
                self.config_manager.config.write(f)
            
            # Reload config in ConfigManager instance
            self.config_manager.load_config() # This re-reads the file into self.config
            self.config_manager._load_search_bot_config() # This updates the specific attributes

            # Re-initialize SearchCacheService with new config
            # Pass the existing maxsize if it was set, or default. For now, use default.
            current_maxsize = self.cache_service.get_cache_stats().get('maxsize', 200) if self.cache_service.is_cache_enabled() else 200

            self.cache_service = SearchCacheService(self.config_manager, maxsize=current_maxsize)
            self.active_full_fetches.clear() # Clear ongoing fetches

            await event.respond(f"âœ… é…ç½®é¡¹ `{key}` å·²æ›´æ–°ä¸º `{processed_value}`ã€‚\næœç´¢ç¼“å­˜å·²ä½¿ç”¨æ–°é…ç½®é‡æ–°åˆå§‹åŒ–ã€‚è¿›è¡Œä¸­çš„å¼‚æ­¥è·å–ä»»åŠ¡å·²æ¸…é™¤ã€‚", parse_mode='md')
            logger.info(f"ç®¡ç†å‘˜ {(await event.get_sender()).id} æ›´æ–°æœç´¢é…ç½®: {key} = {processed_value}")

        except ValueError as ve:
            await event.respond(f"âš ï¸ å€¼é”™è¯¯: {str(ve)}")
        except Exception as e:
            logger.error(f"å¤„ç† /set_search_config å‘½ä»¤æ—¶å‡ºé”™: {e}", exc_info=True)
            await event.respond(f"âš ï¸ æ›´æ–°æœç´¢é…ç½®æ—¶å‡ºç°ä¸¥é‡é”™è¯¯: {str(e)}")

    async def clear_search_cache_command(self, event) -> None:
        """å¤„ç† /clear_search_cache å‘½ä»¤ (ç®¡ç†å‘˜æƒé™)"""
        if not await self.is_admin(event):
            await event.respond("âš ï¸ æ­¤å‘½ä»¤éœ€è¦ç®¡ç†å‘˜æƒé™ã€‚")
            return
        try:
            self.cache_service.clear_cache()
            self.active_full_fetches.clear() # Clear any ongoing background fetch tasks
            await event.respond("âœ… æœç´¢ç¼“å­˜å·²æ¸…ç©ºï¼Œè¿›è¡Œä¸­çš„å¼‚æ­¥è·å–ä»»åŠ¡å·²æ¸…é™¤ã€‚")
            logger.info(f"ç®¡ç†å‘˜ {(await event.get_sender()).id} æ¸…ç©ºäº†æœç´¢ç¼“å­˜")
        except Exception as e:
            logger.error(f"å¤„ç† /clear_search_cache å‘½ä»¤æ—¶å‡ºé”™: {e}")
            await event.respond(f"âš ï¸ æ¸…ç©ºæœç´¢ç¼“å­˜æ—¶å‡ºç°é”™è¯¯: {str(e)}")

    async def view_dialogs_cache_command(self, event) -> None:
        """å¤„ç† /view_dialogs_cache å‘½ä»¤ (ç®¡ç†å‘˜æƒé™)"""
        if not await self.is_admin(event):
            await event.respond("âš ï¸ æ­¤å‘½ä»¤éœ€è¦ç®¡ç†å‘˜æƒé™ã€‚")
            return
        try:
            config_text = "ğŸ’¬ **å¯¹è¯ç¼“å­˜çŠ¶æ€**\n\n"
            
            cache_stats = self.dialogs_cache_service.get_cache_stats()
            if cache_stats.get("enabled"):
                config_text += f"- å¯ç”¨çŠ¶æ€: `å·²å¯ç”¨`\n"
                config_text += f"- ç¼“å­˜TTL: `{cache_stats.get('ttl', 'N/A')}ç§’` (30åˆ†é’Ÿ)\n"
                config_text += f"- å½“å‰æ¡ç›®æ•°: `{cache_stats.get('currsize', 'N/A')}`\n"
                config_text += f"- æœ€å¤§æ¡ç›®æ•°: `{cache_stats.get('maxsize', 'N/A')}`\n\n"
                config_text += "**è¯´æ˜:**\n"
                config_text += "- æ¯ä¸ªç”¨æˆ·çš„å¯¹è¯åˆ—è¡¨å•ç‹¬ç¼“å­˜\n"
                config_text += "- ç¼“å­˜æœ‰æ•ˆæœŸä¸º30åˆ†é’Ÿ\n"
                config_text += "- ç¼“å­˜å¯å‡å°‘å¯¹Telegram APIçš„è°ƒç”¨é¢‘ç‡"
            else:
                config_text += "**çŠ¶æ€:** `å·²ç¦ç”¨`\n\n"
                config_text += "å¯¹è¯ç¼“å­˜å½“å‰å·²ç¦ç”¨ï¼Œæ¯æ¬¡è¯·æ±‚éƒ½ä¼šç›´æ¥è°ƒç”¨Telegram APIã€‚"
            
            await event.respond(config_text, parse_mode='md')
            logger.info(f"ç®¡ç†å‘˜ {(await event.get_sender()).id} æŸ¥çœ‹å¯¹è¯ç¼“å­˜çŠ¶æ€")
        except Exception as e:
            logger.error(f"å¤„ç† /view_dialogs_cache å‘½ä»¤æ—¶å‡ºé”™: {e}")
            await event.respond(f"âš ï¸ æŸ¥çœ‹å¯¹è¯ç¼“å­˜çŠ¶æ€æ—¶å‡ºç°é”™è¯¯: {str(e)}")

    async def clear_dialogs_cache_command(self, event) -> None:
        """å¤„ç† /clear_dialogs_cache å‘½ä»¤ (ç®¡ç†å‘˜æƒé™)"""
        if not await self.is_admin(event):
            await event.respond("âš ï¸ æ­¤å‘½ä»¤éœ€è¦ç®¡ç†å‘˜æƒé™ã€‚")
            return
        try:
            if not self.dialogs_cache_service.is_cache_enabled():
                await event.respond("â„¹ï¸ å¯¹è¯ç¼“å­˜å½“å‰å·²ç¦ç”¨ï¼Œæ— éœ€æ¸…ç©ºã€‚")
                return
            
            # è·å–æ¸…ç©ºå‰çš„ç»Ÿè®¡ä¿¡æ¯
            stats_before = self.dialogs_cache_service.get_cache_stats()
            cleared_count = stats_before.get('currsize', 0)
            
            self.dialogs_cache_service.clear_cache()
            
            await event.respond(f"âœ… å¯¹è¯ç¼“å­˜å·²æ¸…ç©ºã€‚\n\nå·²æ¸…é™¤ `{cleared_count}` ä¸ªç¼“å­˜æ¡ç›®ã€‚", parse_mode='md')
            logger.info(f"ç®¡ç†å‘˜ {(await event.get_sender()).id} æ¸…ç©ºäº†å¯¹è¯ç¼“å­˜ï¼Œæ¸…é™¤äº† {cleared_count} ä¸ªæ¡ç›®")
        except Exception as e:
            logger.error(f"å¤„ç† /clear_dialogs_cache å‘½ä»¤æ—¶å‡ºé”™: {e}")
            await event.respond(f"âš ï¸ æ¸…ç©ºå¯¹è¯ç¼“å­˜æ—¶å‡ºç°é”™è¯¯: {str(e)}")
            
    async def set_oldest_sync_time_command(self, event) -> None:
        """
        å¤„ç† /set_oldest_sync_time å‘½ä»¤ (ç®¡ç†å‘˜æƒé™)
        
        è®¾ç½®å…¨å±€æˆ–ç‰¹å®šèŠå¤©çš„æœ€æ—§åŒæ­¥æ—¶é—´æˆ³
        æ ¼å¼: /set_oldest_sync_time [chat_id] <timestamp>
        å¦‚æœä¸æä¾›chat_idï¼Œåˆ™è®¾ç½®å…¨å±€æ—¶é—´æˆ³
        
        Args:
            event: Telethon äº‹ä»¶å¯¹è±¡
        """
        try:
            # æ£€æŸ¥æƒé™
            if not await self.is_admin(event):
                await event.respond("âš ï¸ æ­¤å‘½ä»¤éœ€è¦ç®¡ç†å‘˜æƒé™ã€‚")
                return
            
            # è·å–å‚æ•°
            message_text = event.message.text
            match = re.match(r"^/set_oldest_sync_time(?:\s+(-?\d+))?(?:\s+(.+))?$", message_text)
            
            # å¸®åŠ©æ–‡æœ¬
            help_text = """**è®¾ç½®æœ€æ—§åŒæ­¥æ—¶é—´æˆ³**
            
ç”¨æ³•:
1. è®¾ç½®å…¨å±€æœ€æ—§åŒæ­¥æ—¶é—´ï¼š
   `/set_oldest_sync_time 2023-01-01T00:00:00Z`

2. è®¾ç½®ç‰¹å®šèŠå¤©çš„æœ€æ—§åŒæ­¥æ—¶é—´ï¼š
   `/set_oldest_sync_time -1001234567890 2023-01-01T00:00:00Z`

3. ç§»é™¤è®¾ç½®ï¼š
   `/set_oldest_sync_time remove` (ç§»é™¤å…¨å±€è®¾ç½®)
   `/set_oldest_sync_time -1001234567890 remove` (ç§»é™¤ç‰¹å®šèŠå¤©è®¾ç½®)

æ—¶é—´æˆ³æ ¼å¼å¯ä»¥æ˜¯ï¼š
- ISO 8601æ—¥æœŸæ—¶é—´ (ä¾‹å¦‚: `2023-01-01T00:00:00Z`)
- Unixæ—¶é—´æˆ³ (ä¾‹å¦‚: `1672531200`)

æ—©äºè¯¥æ—¶é—´æˆ³çš„å†å²æ¶ˆæ¯å°†ä¸ä¼šè¢«åŒæ­¥ã€‚"""
            
            if not match or (not match.group(1) and not match.group(2)):
                # å¦‚æœæ²¡æœ‰æä¾›ä»»ä½•å‚æ•°ï¼Œæ˜¾ç¤ºå¸®åŠ©
                await event.respond(help_text, parse_mode='md')
                return
            
            chat_id_str = match.group(1)
            timestamp_str = match.group(2)
            
            # å¤„ç†åœºæ™¯: /set_oldest_sync_time <timestamp> (å…¨å±€è®¾ç½®)
            if chat_id_str and not timestamp_str:
                # ç¬¬ä¸€ä¸ªå‚æ•°å¯èƒ½æ˜¯æ—¶é—´æˆ³ï¼Œè€Œä¸æ˜¯chat_id
                if not chat_id_str.startswith('-') or not chat_id_str[1:].isdigit():
                    timestamp_str = chat_id_str
                    chat_id = None
                else:
                    # æ˜¯chat_idä½†æ²¡æœ‰æä¾›æ—¶é—´æˆ³
                    await event.respond("è¯·æä¾›æ—¶é—´æˆ³ï¼Œä¾‹å¦‚: `/set_oldest_sync_time -1001234567890 2023-01-01T00:00:00Z`", parse_mode='md')
                    return
            else:
                # å¤„ç†åœºæ™¯: /set_oldest_sync_time <chat_id> <timestamp>
                chat_id = int(chat_id_str) if chat_id_str else None
            
            # å¤„ç†æ—¶é—´æˆ³
            timestamp = None
            if timestamp_str and timestamp_str.lower() != 'remove':
                # å°è¯•è§£æä¸ºISO 8601æ ¼å¼
                try:
                    if timestamp_str.isdigit():
                        # æ˜¯Unixæ—¶é—´æˆ³
                        timestamp = int(timestamp_str)
                    else:
                        # æ˜¯ISO 8601æ ¼å¼
                        timestamp = timestamp_str
                except ValueError:
                    await event.respond(f"âš ï¸ æ— æ•ˆçš„æ—¶é—´æˆ³æ ¼å¼: `{timestamp_str}`\n\n{help_text}", parse_mode='md')
                    return
            
            # æ‰§è¡Œè®¾ç½®
            success = self.config_manager.set_oldest_sync_timestamp(chat_id, timestamp)
            
            if success:
                if chat_id is None:
                    if timestamp is None:
                        message = "âœ… å·²æˆåŠŸç§»é™¤å…¨å±€æœ€æ—§åŒæ­¥æ—¶é—´æˆ³è®¾ç½®ã€‚"
                    else:
                        message = f"âœ… å·²è®¾ç½®å…¨å±€æœ€æ—§åŒæ­¥æ—¶é—´æˆ³ä¸º: `{timestamp}`"
                else:
                    if timestamp is None:
                        message = f"âœ… å·²æˆåŠŸç§»é™¤èŠå¤© `{chat_id}` çš„æœ€æ—§åŒæ­¥æ—¶é—´æˆ³è®¾ç½®ã€‚"
                    else:
                        message = f"âœ… å·²è®¾ç½®èŠå¤© `{chat_id}` çš„æœ€æ—§åŒæ­¥æ—¶é—´æˆ³ä¸º: `{timestamp}`"
                
                await event.respond(message, parse_mode='md')
                logger.info(f"ç®¡ç†å‘˜ {(await event.get_sender()).id} è®¾ç½®æœ€æ—§åŒæ­¥æ—¶é—´æˆ³: chat_id={chat_id}, timestamp={timestamp}")
            else:
                await event.respond("âš ï¸ è®¾ç½®æœ€æ—§åŒæ­¥æ—¶é—´æˆ³å¤±è´¥ï¼Œè¯·æ£€æŸ¥å‚æ•°æ ¼å¼ã€‚", parse_mode='md')
                
        except Exception as e:
            logger.error(f"å¤„ç† /set_oldest_sync_time å‘½ä»¤æ—¶å‡ºé”™: {e}")
            await event.respond(f"âš ï¸ è®¾ç½®æœ€æ—§åŒæ­¥æ—¶é—´æˆ³æ—¶å‡ºç°é”™è¯¯: {str(e)}")
    
    async def view_oldest_sync_time_command(self, event) -> None:
        """
        å¤„ç† /view_oldest_sync_time å‘½ä»¤ (ç®¡ç†å‘˜æƒé™)
        
        æŸ¥çœ‹å…¨å±€æˆ–ç‰¹å®šèŠå¤©çš„æœ€æ—§åŒæ­¥æ—¶é—´æˆ³
        æ ¼å¼: /view_oldest_sync_time [chat_id]
        å¦‚æœä¸æä¾›chat_idï¼Œåˆ™æ˜¾ç¤ºæ‰€æœ‰è®¾ç½®
        
        Args:
            event: Telethon äº‹ä»¶å¯¹è±¡
        """
        try:
            # æ£€æŸ¥æƒé™
            if not await self.is_admin(event):
                await event.respond("âš ï¸ æ­¤å‘½ä»¤éœ€è¦ç®¡ç†å‘˜æƒé™ã€‚")
                return
            
            # è·å–å‚æ•°
            message_text = event.message.text
            match = re.match(r"^/view_oldest_sync_time(?:\s+(-?\d+))?$", message_text)
            
            chat_id_str = match.group(1) if match else None
            
            if chat_id_str:
                # æŸ¥çœ‹ç‰¹å®šèŠå¤©çš„è®¾ç½®
                chat_id = int(chat_id_str)
                timestamp = self.config_manager.get_oldest_sync_timestamp(chat_id)
                
                response = f"ğŸ•’ **èŠå¤© `{chat_id}` çš„æœ€æ—§åŒæ­¥æ—¶é—´è®¾ç½®**\n\n"
                if timestamp:
                    response += f"æœ€æ—§åŒæ­¥æ—¶é—´æˆ³: `{timestamp.isoformat()}`\n"
                    response += f"Unixæ—¶é—´æˆ³: `{int(timestamp.timestamp())}`\n\n"
                    response += "æ—©äºæ­¤æ—¶é—´çš„æ¶ˆæ¯å°†ä¸ä¼šè¢«åŒæ­¥ã€‚"
                else:
                    response += "æ­¤èŠå¤©æ²¡æœ‰ç‰¹å®šçš„æœ€æ—§åŒæ­¥æ—¶é—´è®¾ç½®ï¼Œå°†ä½¿ç”¨å…¨å±€è®¾ç½®ï¼ˆå¦‚æœæœ‰ï¼‰ã€‚"
            else:
                # æŸ¥çœ‹æ‰€æœ‰è®¾ç½®
                sync_settings = getattr(self.config_manager, 'sync_settings', {}) or {}
                
                response = "ğŸ•’ **æœ€æ—§åŒæ­¥æ—¶é—´è®¾ç½®**\n\n"
                
                # æ˜¾ç¤ºå…¨å±€è®¾ç½®
                if "global_oldest_sync_timestamp" in sync_settings:
                    global_timestamp = sync_settings["global_oldest_sync_timestamp"]
                    response += f"**å…¨å±€è®¾ç½®**: `{global_timestamp}`\n\n"
                else:
                    response += "**å…¨å±€è®¾ç½®**: æœªè®¾ç½®\n\n"
                
                # æ˜¾ç¤ºç‰¹å®šèŠå¤©è®¾ç½®
                chat_settings = [k for k in sync_settings.keys() if k != "global_oldest_sync_timestamp"]
                if chat_settings:
                    response += "**èŠå¤©ç‰¹å®šè®¾ç½®**:\n"
                    for chat_id_key in chat_settings:
                        try:
                            chat_id = int(chat_id_key)
                            if isinstance(sync_settings[chat_id_key], dict) and "oldest_sync_timestamp" in sync_settings[chat_id_key]:
                                chat_timestamp = sync_settings[chat_id_key]["oldest_sync_timestamp"]
                                response += f"- èŠå¤© `{chat_id}`: `{chat_timestamp}`\n"
                        except (ValueError, TypeError):
                            continue
                else:
                    response += "**èŠå¤©ç‰¹å®šè®¾ç½®**: æ— \n"
            
            await event.respond(response, parse_mode='md')
            logger.info(f"ç®¡ç†å‘˜ {(await event.get_sender()).id} æŸ¥çœ‹æœ€æ—§åŒæ­¥æ—¶é—´è®¾ç½®")
            
        except Exception as e:
            logger.error(f"å¤„ç† /view_oldest_sync_time å‘½ä»¤æ—¶å‡ºé”™: {e}", exc_info=True)
            await event.respond(f"âš ï¸ æŸ¥çœ‹æœ€æ—§åŒæ­¥æ—¶é—´è®¾ç½®æ—¶å‡ºç°é”™è¯¯: {str(e)}")

# è¾…åŠ©å‡½æ•°ï¼šåˆ›å»ºå‘½ä»¤å¤„ç†å™¨å¹¶æ³¨å†Œåˆ°å®¢æˆ·ç«¯
def setup_command_handlers(
    client,
    meilisearch_service: MeiliSearchService,
    config_manager: ConfigManager,
    admin_ids: List[int],
    userbot_restart_event: Optional[asyncio.Event] = None
) -> CommandHandlers:
    """
    åˆ›å»ºå‘½ä»¤å¤„ç†å™¨å¹¶å°†å…¶æ³¨å†Œåˆ°å®¢æˆ·ç«¯
    
    Args:
        client: Telethon å®¢æˆ·ç«¯
        meilisearch_service: Meilisearch æœåŠ¡å®ä¾‹
        config_manager: é…ç½®ç®¡ç†å™¨å®ä¾‹
        admin_ids: ç®¡ç†å‘˜ç”¨æˆ· ID åˆ—è¡¨
        userbot_restart_event: User Bot é‡å¯äº‹ä»¶ï¼Œç”¨äºè§¦å‘é‡å¯
        
    Returns:
        CommandHandlers: å‘½ä»¤å¤„ç†å™¨å®ä¾‹
    """
    handler = CommandHandlers(
        client=client,
        meilisearch_service=meilisearch_service,
        config_manager=config_manager,
        admin_ids=admin_ids,
        userbot_restart_event=userbot_restart_event
    )
    
    return handler